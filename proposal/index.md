# Research proposal

Given the availability of the ASL database as a global sign language dataset and the need to customize personal datasets for sufficient guessability and high subjective ratings of user-defined gesture sets to produce suitable, context-dependent sentences, how can the design of multimodal input interfaces be optimized to support seamless transitions between different input modes, including tap, motion, and image gestures, and enhance the overall user experience?

For related questions refer to the [questions](questions.md) file.