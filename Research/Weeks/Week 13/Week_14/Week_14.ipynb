{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m------\n",
      "Starting Potassium Server 🍌\u001b[0m\n",
      "\u001b[33mRunning init()\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mStarted 1 workers\u001b[0m\n",
      "\u001b[32mServing at http://0.0.0.0:8000\n",
      "------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from potassium import Potassium, Request, Response\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "\n",
    "app = Potassium(\"my_app\")\n",
    "\n",
    "# @app.init runs at startup, and initializes the app's context\n",
    "@app.init\n",
    "def init():\n",
    "    # Check if there is a GPU\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "    # Defind the model, using the \"pipeline\" interface from transformers\n",
    "    bertModel = pipeline('fill-mask', model='bert-base-uncased', device=device)\n",
    "   \n",
    "    \n",
    "    context = {\n",
    "        \"model_b\": bertModel,\n",
    "    }\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "\n",
    "# @app.handler is an http post handler running for every call\n",
    "@app.handler(\"/bert\")\n",
    "def handler(context: dict, request: Request) -> Response:\n",
    "    \n",
    "    # our model is from 'model_b' as defined in the context earlier\n",
    "    model = context.get(\"model_b\")\n",
    "\n",
    "    # our prompt is from the 'prompt' key in the user request\n",
    "    prompt = request.json.get(\"prompt\")\n",
    "\n",
    "    # the outputs is created by passing the user request to the model\n",
    "    outputs = model(prompt)\n",
    "\n",
    "    # returned\n",
    "    return Response(\n",
    "        json = {\"outputs\": outputs}, \n",
    "        status=200\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.serve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m------\n",
      "Starting Potassium Server 🍌\u001b[0m\n",
      "\u001b[33mRunning init()\u001b[0m\n",
      "\u001b[32mStarted 1 workers\u001b[0m\n",
      "\u001b[32mServing at http://0.0.0.0:8000\n",
      "------\u001b[0m\n",
      "127.0.0.1 - - [22/Mar/2024 09:05:44] \"\u001b[31m\u001b[1mGET / HTTP/1.1\u001b[0m\" 405 -\n",
      "127.0.0.1 - - [22/Mar/2024 09:05:45] \"\u001b[31m\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" 405 -\n",
      "127.0.0.1 - - [22/Mar/2024 09:05:58] \"\u001b[31m\u001b[1mGET / HTTP/1.1\u001b[0m\" 405 -\n"
     ]
    }
   ],
   "source": [
    "from potassium import Potassium, Request, Response\n",
    "# from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "app = Potassium(\"my_app\")\n",
    "\n",
    "def custom_model(inputData):\n",
    "  outcome = \"Hello from a function, this is your input data: \" + inputData\n",
    "  print(outcome) \n",
    "  return outcome\n",
    "\n",
    "# @app.init runs at startup, and initializes the app's context\n",
    "@app.init\n",
    "def init():\n",
    "\n",
    "    # Check if there is a GPU\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "    # Define the model, using the \"pipeline\" interface from transformers\n",
    "    # bertModel = pipeline('fill-mask', model='bert-base-uncased', device=device)\n",
    "\n",
    "    # define any model that you understand how to process\n",
    "    # model = custom_model\n",
    "    \n",
    "    context = {\n",
    "        # \"model_b\": bertModel,\n",
    "        # model: model\n",
    "    }\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "\n",
    "# @app.handler is an http post handler running for every call\n",
    "@app.handler(\"/custom\")\n",
    "def handler(context: dict, request: Request) -> Response:\n",
    "    \n",
    "    # our model is from 'model_b' as defined in the context earlier\n",
    "    # model = context.get(\"model\")\n",
    "    # model = custom_model\n",
    "\n",
    "    # our prompt is from the 'prompt' key in the user request\n",
    "    # input = request.json.get(\"prompt\")\n",
    "\n",
    "    inputData = 'this is where my input data will be.'\n",
    "\n",
    "    # the outputs is created by passing the user request to the model\n",
    "    outcome = custom_model(inputData)\n",
    "\n",
    "    print(outcome)\n",
    "\n",
    "    # returned\n",
    "    return Response(\n",
    "        json = {\"outputs\": outcome}, \n",
    "        status=200\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.serve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
