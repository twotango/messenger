{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4r/5qs7726s3_dc54vtgh1sc2740000gn/T/ipykernel_2413/1207623996.py:3: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  smartphone = pd.read_csv('/Users/emilkoch/Desktop/2Tango/Data Files/Dataset_2_glasses/pivot_table_s_split.csv')\n"
     ]
    }
   ],
   "source": [
    "pivot_data4 = pd.read_csv('/Users/emilkoch/Desktop/2Tango/Data Files/Dataset_2_glasses/pivot_data4.csv')\n",
    "activity_type_full = pd.read_csv('/Users/emilkoch/Desktop/2Tango/Data Files/Dataset_2_glasses/report_full_activity_type.csv')\n",
    "smartphone = pd.read_csv('/Users/emilkoch/Desktop/2Tango/Data Files/Dataset_2_glasses/pivot_table_s_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_motion_row(row):\n",
    "    # Extract relevant data from the row\n",
    "    accel_x = row['accel_x']\n",
    "    accel_y = row['accel_y']\n",
    "    accel_z = row['accel_z']\n",
    "    gyro_x = row['gyro_x']\n",
    "    gyro_y = row['gyro_y']\n",
    "    gyro_z = row['gyro_z']\n",
    "\n",
    "    # Calculate magnitudes along x, y, and z axes\n",
    "    accel_mag_x_y = np.sqrt(accel_x**2 + accel_y**2)\n",
    "    accel_mag_z = np.abs(accel_z)\n",
    "    \n",
    "    # Classify motion based on accelerometer and gyroscope readings\n",
    "    if np.any((accel_x != 0) | (accel_y != 0) | (accel_z != 0)):\n",
    "        # Check if all gyroscope readings are zero\n",
    "        if not np.any((gyro_x != 0) | (gyro_y != 0) | (gyro_z != 0)):\n",
    "            # Check if there is significant upward or downward motion along the z-axis\n",
    "            if accel_z > 0 and accel_mag_x_y > accel_mag_z:\n",
    "                motion_type = \"Upward Horizontal Motion\"\n",
    "            elif accel_z < 0 and accel_mag_x_y > accel_mag_z:\n",
    "                motion_type = \"Downward Horizontal Motion\"\n",
    "            else:\n",
    "                motion_type = \"Other Moving\"\n",
    "        else:\n",
    "            # If gyroscope readings are not all zero, classify as \"Turning/Tilting\"\n",
    "            motion_type = \"Turning/Tilting\"\n",
    "    elif np.any((gyro_x != 0) | (gyro_y != 0) | (gyro_z != 0)):\n",
    "        # If accelerometer readings are all zero, check gyroscope readings\n",
    "        motion_type = \"Turning/Tilting\"\n",
    "    else:\n",
    "        # If both accelerometer and gyroscope readings are zero, classify as \"Stationary\"\n",
    "        motion_type = \"Stationary\"\n",
    "\n",
    "    return pd.Series([motion_type])\n",
    "\n",
    "# Apply the function row-wise to the dataframe\n",
    "pivot_data4['motion_type'] = pivot_data4.apply(classify_motion_row, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Create and fit the LabelEncoder\n",
    "label_encoder_motion = LabelEncoder()\n",
    "pivot_data4['motion_encoded'] = label_encoder_motion.fit_transform(pivot_data4['motion_type'])\n",
    "\n",
    "# Save the LabelEncoder to a file\n",
    "joblib.dump(label_encoder_motion, '/Users/emilkoch/Desktop/2Tango/messenger/Research/Job_Library/label_encoder_motion.pkl')\n",
    "\n",
    "# Get the encodings and decodings\n",
    "encodings = label_encoder_motion.transform(label_encoder_motion.classes_)\n",
    "decodings = label_encoder_motion.classes_\n",
    "\n",
    "# Display the encodings and decodings\n",
    "for encoding, decoding in zip(encodings, decodings):\n",
    "    print(f\"Encoding: {encoding}, Decoding: {decoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = pivot_data4[['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']]\n",
    "y = pivot_data4['motion_encoded']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model with L1 regularization\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.001), input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(len(np.unique(pivot_data4['motion_encoded'])), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Encode class labels\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "# Compute class weights using the encoded labels\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "\n",
    "# Create a class weight dictionary\n",
    "class_weight_dict = dict(zip(le.transform(le.classes_), class_weights))\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model with class weights and early stopping\n",
    "history = model.fit(X_train_scaled, y_train_encoded, epochs=20, validation_data=(X_val_scaled, y_val), class_weight=class_weight_dict, callbacks=[early_stopping])\n",
    "\n",
    "# Save the model\n",
    "file_path = \"/Users/emilkoch/Desktop/2Tango/messenger/research/Model_Save/motion.keras\"\n",
    "save_model(model, file_path)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartphone = smartphone.drop(columns = ['activity_type'])\n",
    "# Convert timestamp columns to datetime objects\n",
    "activity_type_full['timestamp'] = pd.to_datetime(activity_type_full['timestamp'])\n",
    "smartphone['timestamp'] = pd.to_datetime(smartphone['timestamp'])\n",
    "# Convert timestamp columns to datetime objects\n",
    "activity_type_full['timestamp'] = pd.to_datetime(activity_type_full['timestamp'])\n",
    "smartphone['timestamp'] = pd.to_datetime(smartphone['timestamp'])\n",
    "\n",
    "# Group activity_type_full by timestamp and transform it to align with smartphone\n",
    "activity_type_grouped = activity_type_full.groupby(smartphone['timestamp'])['activity_type'].apply(lambda x: ','.join(x)).reset_index()\n",
    "\n",
    "# Merge activity_type_grouped with smartphone based on timestamp\n",
    "smartphone = pd.merge(smartphone, activity_type_grouped, on='timestamp', how='left')\n",
    "\n",
    "# Print or further process the updated DataFrame\n",
    "print(smartphone)\n",
    "\n",
    "smartphone.fillna(0, inplace=True)\n",
    "\n",
    "# Filter the DataFrame to keep rows where \"activity_type\" is not equal to 0\n",
    "merged_data_s_f = smartphone[smartphone['activity_type'] != 0]\n",
    "# Assuming 'pivot_data4' is your DataFrame containing the timestamp column\n",
    "merged_data_s_f['timestamp'] = pd.to_datetime(merged_data_s_f['timestamp'])\n",
    "\n",
    "# Extract hour and minute from the timestamp and convert to minutes\n",
    "merged_data_s_f['timestamp'] = merged_data_s_f['timestamp'].dt.hour * 60 + merged_data_s_f['timestamp'].dt.minute\n",
    "\n",
    "merged_data_s_f_filtered = merged_data_s_f[['timestamp', 'activity_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'pivot_data4' is your DataFrame containing the timestamp column\n",
    "pivot_data4['timestamp'] = pd.to_datetime(pivot_data4['timestamp'])\n",
    "# Extract hour and minute from the timestamp and convert to minutes\n",
    "pivot_data4['timestamp'] = pivot_data4['timestamp'].dt.hour * 60 + pivot_data4['timestamp'].dt.minute\n",
    "# Merge the dataframes on the 'timestamp' column\n",
    "pivot_data4_merged = pivot_data4.merge(merged_data_s_f_filtered, on='timestamp', how='left')\n",
    "pivot_data4_merged.fillna(0, inplace=True)\n",
    "pivot_data4_merged_filtered = pivot_data4_merged[pivot_data4_merged['activity_type'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create and fit the LabelEncoder\n",
    "label_encoder_activities = LabelEncoder()\n",
    "pivot_data4_merged_filtered['activity_encoded'] = label_encoder_activities.fit_transform(pivot_data4_merged_filtered['activity_type'])\n",
    "\n",
    "# Save the LabelEncoder to a file\n",
    "joblib.dump(label_encoder_activities, '/Users/emilkoch/Desktop/2Tango/messenger/Research/Job_Library/label_encoder_activities.pkl')\n",
    "\n",
    "# Get unique classes from the LabelEncoder\n",
    "unique_classes = label_encoder_activities.classes_\n",
    "\n",
    "# Print unique classes\n",
    "print(\"Unique Classes:\")\n",
    "for class_ in unique_classes:\n",
    "    print(class_)\n",
    "\n",
    "# Get the encodings and decodings\n",
    "encodings = label_encoder_activities.transform(label_encoder_activities.classes_)\n",
    "decodings = label_encoder_activities.classes_\n",
    "\n",
    "# Display the encodings and decodings\n",
    "for encoding, decoding in zip(encodings, decodings):\n",
    "    print(f\"Encoding: {encoding}, Decoding: {decoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_activity(row):\n",
    "    motion_type = row['motion_type']\n",
    "    if motion_type == \"Downward Horizontal Motion\":\n",
    "        return [13, 20, 21, 3, 6, 18, 4]\n",
    "    elif motion_type == \"Upward Horizontal Motion\":\n",
    "        return [6, 18, 4, 13, 20, 21, 3]\n",
    "    elif motion_type == \"Other Moving\":\n",
    "        return [1, 7, 8, 15, 6, 18, 4, 13, 20, 21, 3]\n",
    "    elif motion_type == \"Turning/Tilting\":\n",
    "        return [11, 17]\n",
    "    elif motion_type == \"Stationary\":\n",
    "        return [0, 10, 14, 23, 2, 16, 22, 12, 19, 5, 9]\n",
    "\n",
    "# Apply the function to create the predicted_activity column\n",
    "pivot_data4['predicted_activity'] = pivot_data4.apply(classify_activity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_motion_activity(row):\n",
    "    activity_encoded = row['activity_encoded']\n",
    "    if activity_encoded in [13, 20, 21, 3, 6, 18, 4]:\n",
    "        true_motion = 0\n",
    "    elif activity_encoded in [6, 18, 4, 13, 20, 21, 3]:\n",
    "        true_motion = 4\n",
    "    elif activity_encoded in [1, 7, 8, 15, 6, 18, 4, 13, 20, 21, 3]:\n",
    "        true_motion = 1\n",
    "    elif activity_encoded in [11, 17]:\n",
    "        true_motion = 3\n",
    "    elif activity_encoded in [0, 10, 14, 23, 2, 16, 22, 12, 19, 5, 9]:\n",
    "        true_motion = 2\n",
    "    return true_motion\n",
    "\n",
    "# Assuming 'pivot_data4' is your DataFrame\n",
    "pivot_data4_merged_filtered['true_motion'] = pivot_data4_merged_filtered.apply(classify_motion_activity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = pivot_data4[['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z', 'motion_encoded', 'timestamp']]\n",
    "y = pivot_data4['predicted_activity']\n",
    "\n",
    "# Convert labels into binary format for multi-label classification\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_encoded = mlb.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(len(mlb.classes_), activation='sigmoid')  # Sigmoid activation for multi-label classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10, validation_split=0.2, verbose=1, callbacks=[EarlyStopping(patience=5)])\n",
    "\n",
    "# Save the model\n",
    "file_path = \"/Users/emilkoch/Desktop/2Tango/messenger/research/Model_Save/activities.keras\"\n",
    "save_model(model, file_path)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "y_pred_labels = mlb.inverse_transform(y_pred)\n",
    "\n",
    "# Convert ground truth labels back to original format\n",
    "y_true_labels = mlb.inverse_transform(y_test)\n",
    "\n",
    "# Convert ground truth labels to binary array format\n",
    "y_true_binary = mlb.transform(y_true_labels)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true_binary, y_pred, target_names=mlb.classes_.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = pivot_data4[['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z', 'motion_encoded', 'timestamp']]\n",
    "y = pivot_data4['predicted_activity']\n",
    "\n",
    "# Convert labels into binary format for multi-label classification\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_encoded = mlb.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# Initialize a multi-output classifier with GradientBoostingClassifier as the base estimator\n",
    "multi_gb_model = MultiOutputClassifier(GradientBoostingClassifier())\n",
    "\n",
    "# Train the model\n",
    "multi_gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy_gb = multi_gb_model.score(X_test_scaled, y_test)\n",
    "print(\"Test Accuracy (Gradient Boosting):\", test_accuracy_gb)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = multi_gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_gb, target_names=mlb.classes_.astype(str)))\n",
    "\n",
    "# Define the file path to save the model\n",
    "model_file_path = \"/Users/emilkoch/Desktop/2Tango/messenger/Research/Model_Save/multi_gb_model.pkl\"\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(multi_gb_model, model_file_path)\n",
    "\n",
    "print(\"Model saved successfully at:\", model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Load the previously trained model\n",
    "model = load_model(\"/Users/emilkoch/Desktop/2Tango/messenger/research/Model_Save/activities.keras\")\n",
    "\n",
    "# Define the new target variable (y) using single true labels\n",
    "y_single_true_labels = pivot_data4_merged_filtered['activity_encoded']\n",
    "\n",
    "# Convert labels into binary format for single-label classification\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_single_true_binary = label_binarizer.fit_transform(y_single_true_labels)\n",
    "\n",
    "# Filter X to include only rows with true labels\n",
    "X_filtered = X.loc[y_single_true_labels.index]\n",
    "\n",
    "# Split the filtered data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_single_true_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Retrain the model with class weights\n",
    "history = model.fit(X_train_scaled, y_train, epochs=20, validation_split=0.2, verbose=1, callbacks=[EarlyStopping(patience=5)])\n",
    "\n",
    "# Save the model\n",
    "file_path = \"/Users/emilkoch/Desktop/2Tango/messenger/research/Model_Save/activities.keras\"\n",
    "save_model(model, file_path)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred, target_names=label_binarizer.classes_.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"/Users/emilkoch/Desktop/2Tango/messenger/Research/Model_Save/multi_gb_model.pkl\")\n",
    "\n",
    "# Define the new target variable (y) using single true labels\n",
    "y_single_true_labels = pivot_data4_merged_filtered['activity_encoded']\n",
    "\n",
    "# Convert labels into binary format for single-label classification\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_single_true_binary = label_binarizer.fit_transform(y_single_true_labels)\n",
    "\n",
    "# Filter X to include only rows with true labels\n",
    "X_filtered = X.loc[y_single_true_labels.index]\n",
    "\n",
    "# Split the filtered data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_single_true_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "multi_gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy_gb = multi_gb_model.score(X_test_scaled, y_test)\n",
    "print(\"Test Accuracy (Gradient Boosting):\", test_accuracy_gb)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = multi_gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_gb, target_names=mlb.classes_.astype(str)))\n",
    "\n",
    "# Define the file path to save the model\n",
    "model_file_path = \"/Users/emilkoch/Desktop/2Tango/messenger/Research/Model_Save/multi_gb_model.pkl\"\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(multi_gb_model, model_file_path)\n",
    "\n",
    "print(\"Model saved successfully at:\", model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"/Users/emilkoch/Desktop/2Tango/messenger/research/Model_Save/motion.keras\")\n",
    "\n",
    "X = pivot_data4_merged_filtered[['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']]\n",
    "\n",
    "# Define the new target variable (y) using single true labels\n",
    "y = pivot_data4_merged_filtered['true_motion']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Encode class labels for the new target variable\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "# Compute class weights using the encoded labels\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "\n",
    "# Create a class weight dictionary\n",
    "class_weight_dict = dict(zip(le.transform(le.classes_), class_weights))\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model with class weights and early stopping\n",
    "history = loaded_model.fit(X_train_scaled, y_train_encoded, epochs=100, batch_size= 45, validation_data=(X_val_scaled, y_val), class_weight=class_weight_dict, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the retrained model on the test set\n",
    "test_loss, test_accuracy = loaded_model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = loaded_model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
