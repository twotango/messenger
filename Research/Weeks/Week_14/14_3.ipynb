{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4r/5qs7726s3_dc54vtgh1sc2740000gn/T/ipykernel_75090/1207623996.py:3: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  smartphone = pd.read_csv('/Users/emilkoch/Desktop/2Tango/Data Files/Dataset_2_glasses/pivot_table_s_split.csv')\n"
     ]
    }
   ],
   "source": [
    "pivot_data4 = pd.read_csv('/Users/emilkoch/Desktop/2Tango/Data Files/Dataset_2_glasses/pivot_data4.csv')\n",
    "activity_type_full = pd.read_csv('/Users/emilkoch/Desktop/2Tango/Data Files/Dataset_2_glasses/report_full_activity_type.csv')\n",
    "smartphone = pd.read_csv('/Users/emilkoch/Desktop/2Tango/Data Files/Dataset_2_glasses/pivot_table_s_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_motion_row(row):\n",
    "    # Extract relevant data from the row\n",
    "    accel_x = row['accel_x']\n",
    "    accel_y = row['accel_y']\n",
    "    accel_z = row['accel_z']\n",
    "    gyro_x = row['gyro_x']\n",
    "    gyro_y = row['gyro_y']\n",
    "    gyro_z = row['gyro_z']\n",
    "\n",
    "    # Calculate magnitudes along x, y, and z axes\n",
    "    accel_mag_x_y = np.sqrt(accel_x**2 + accel_y**2)\n",
    "    accel_mag_z = np.abs(accel_z)\n",
    "    \n",
    "    # Classify motion based on accelerometer and gyroscope readings\n",
    "    if np.any((accel_x != 0) | (accel_y != 0) | (accel_z != 0)):\n",
    "        # Check if all gyroscope readings are zero\n",
    "        if not np.any((gyro_x != 0) | (gyro_y != 0) | (gyro_z != 0)):\n",
    "            # Check if there is significant upward or downward motion along the z-axis\n",
    "            if accel_z > 0 and accel_mag_x_y > accel_mag_z:\n",
    "                motion_type = \"Upward Horizontal Motion\"\n",
    "            elif accel_z < 0 and accel_mag_x_y > accel_mag_z:\n",
    "                motion_type = \"Downward Horizontal Motion\"\n",
    "            else:\n",
    "                motion_type = \"Other Moving\"\n",
    "        else:\n",
    "            # If gyroscope readings are not all zero, classify as \"Turning/Tilting\"\n",
    "            motion_type = \"Turning/Tilting\"\n",
    "    elif np.any((gyro_x != 0) | (gyro_y != 0) | (gyro_z != 0)):\n",
    "        # If accelerometer readings are all zero, check gyroscope readings\n",
    "        motion_type = \"Turning/Tilting\"\n",
    "    else:\n",
    "        # If both accelerometer and gyroscope readings are zero, classify as \"Stationary\"\n",
    "        motion_type = \"Stationary\"\n",
    "\n",
    "    return pd.Series([motion_type])\n",
    "\n",
    "# Apply the function row-wise to the dataframe\n",
    "pivot_data4['motion_type'] = pivot_data4.apply(classify_motion_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding: 0, Decoding: Downward Horizontal Motion\n",
      "Encoding: 1, Decoding: Other Moving\n",
      "Encoding: 2, Decoding: Stationary\n",
      "Encoding: 3, Decoding: Turning/Tilting\n",
      "Encoding: 4, Decoding: Upward Horizontal Motion\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Create and fit the LabelEncoder\n",
    "label_encoder_motion = LabelEncoder()\n",
    "pivot_data4['motion_encoded'] = label_encoder_motion.fit_transform(pivot_data4['motion_type'])\n",
    "\n",
    "# Save the LabelEncoder to a file\n",
    "joblib.dump(label_encoder_motion, '/Users/emilkoch/Desktop/2Tango/messenger/Research/Job_Library/label_encoder_motion.pkl')\n",
    "\n",
    "# Get the encodings and decodings\n",
    "encodings = label_encoder_motion.transform(label_encoder_motion.classes_)\n",
    "decodings = label_encoder_motion.classes_\n",
    "\n",
    "# Display the encodings and decodings\n",
    "for encoding, decoding in zip(encodings, decodings):\n",
    "    print(f\"Encoding: {encoding}, Decoding: {decoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_data4.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     timestamp  step_detector_values  audio_value_1  \\\n",
      "0      2017-06-29 07:45:16.506                   1.0            0.0   \n",
      "1      2017-06-29 07:45:17.407                   0.0            0.0   \n",
      "2      2017-06-29 07:45:18.407                   0.0          227.0   \n",
      "3      2017-06-29 07:45:19.407                   0.0          590.0   \n",
      "4      2017-06-29 07:45:20.446                   0.0         1724.0   \n",
      "...                        ...                   ...            ...   \n",
      "617948 2017-07-13 20:13:33.671                   0.0         1956.0   \n",
      "617949 2017-07-13 20:13:34.663                   0.0         2673.0   \n",
      "617950 2017-07-13 20:13:35.663                   0.0         2340.0   \n",
      "617951 2017-07-13 20:13:36.668                   0.0         2046.0   \n",
      "617952 2017-07-13 20:13:37.668                   0.0         2441.0   \n",
      "\n",
      "        audio_value_2  audio_value_3  audio_value_4 activity_type  \n",
      "0             0.00000            0.0            0.0   Video games  \n",
      "1             0.00000            0.0        32767.0   In computer  \n",
      "2           113.50000          227.0          227.0       At home  \n",
      "3           272.33334          590.0          227.0   In computer  \n",
      "4           635.25000         1724.0          227.0           Eat  \n",
      "...               ...            ...            ...           ...  \n",
      "617948     5246.24850        32767.0           30.0           NaN  \n",
      "617949     5246.18160        32767.0           30.0           NaN  \n",
      "617950     5246.10640        32767.0           30.0           NaN  \n",
      "617951     5246.02400        32767.0           30.0           NaN  \n",
      "617952     5245.95170        32767.0           30.0           NaN  \n",
      "\n",
      "[617953 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4r/5qs7726s3_dc54vtgh1sc2740000gn/T/ipykernel_75090/4048118510.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data_s_f['timestamp'] = pd.to_datetime(merged_data_s_f['timestamp'])\n",
      "/var/folders/4r/5qs7726s3_dc54vtgh1sc2740000gn/T/ipykernel_75090/4048118510.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data_s_f['timestamp'] = merged_data_s_f['timestamp'].dt.hour * 60 + merged_data_s_f['timestamp'].dt.minute\n"
     ]
    }
   ],
   "source": [
    "smartphone = smartphone.drop(columns = ['activity_type'])\n",
    "# Convert timestamp columns to datetime objects\n",
    "activity_type_full['timestamp'] = pd.to_datetime(activity_type_full['timestamp'])\n",
    "smartphone['timestamp'] = pd.to_datetime(smartphone['timestamp'])\n",
    "# Convert timestamp columns to datetime objects\n",
    "activity_type_full['timestamp'] = pd.to_datetime(activity_type_full['timestamp'])\n",
    "smartphone['timestamp'] = pd.to_datetime(smartphone['timestamp'])\n",
    "\n",
    "# Group activity_type_full by timestamp and transform it to align with smartphone\n",
    "activity_type_grouped = activity_type_full.groupby(smartphone['timestamp'])['activity_type'].apply(lambda x: ','.join(x)).reset_index()\n",
    "\n",
    "# Merge activity_type_grouped with smartphone based on timestamp\n",
    "smartphone = pd.merge(smartphone, activity_type_grouped, on='timestamp', how='left')\n",
    "\n",
    "# Print or further process the updated DataFrame\n",
    "print(smartphone)\n",
    "\n",
    "smartphone.fillna(0, inplace=True)\n",
    "\n",
    "# Filter the DataFrame to keep rows where \"activity_type\" is not equal to 0\n",
    "merged_data_s_f = smartphone[smartphone['activity_type'] != 0]\n",
    "# Assuming 'pivot_data4' is your DataFrame containing the timestamp column\n",
    "merged_data_s_f['timestamp'] = pd.to_datetime(merged_data_s_f['timestamp'])\n",
    "\n",
    "# Extract hour and minute from the timestamp and convert to minutes\n",
    "merged_data_s_f['timestamp'] = merged_data_s_f['timestamp'].dt.hour * 60 + merged_data_s_f['timestamp'].dt.minute\n",
    "\n",
    "merged_data_s_f_filtered = merged_data_s_f[['timestamp', 'activity_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'pivot_data4' is your DataFrame containing the timestamp column\n",
    "pivot_data4['timestamp'] = pd.to_datetime(pivot_data4['timestamp'])\n",
    "# Extract hour and minute from the timestamp and convert to minutes\n",
    "pivot_data4['timestamp'] = pivot_data4['timestamp'].dt.hour * 60 + pivot_data4['timestamp'].dt.minute\n",
    "# Merge the dataframes on the 'timestamp' column\n",
    "pivot_data4_merged = pivot_data4.merge(merged_data_s_f_filtered, on='timestamp', how='left')\n",
    "pivot_data4_merged.fillna(0, inplace=True)\n",
    "pivot_data4_merged_filtered = pivot_data4_merged[pivot_data4_merged['activity_type'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Classes:\n",
      "At home\n",
      "Cooking\n",
      "Eat\n",
      "Football\n",
      "In bus\n",
      "In computer\n",
      "In vehicle\n",
      "Meeting\n",
      "Movie\n",
      "On bus stop\n",
      "Pause\n",
      "Phone was out of the pocket (forgot)\n",
      "Picnic \n",
      "Running\n",
      "Shop\n",
      "Shopping& wearing\n",
      "Sleep\n",
      "Took off glasses\n",
      "Train\n",
      "Video games\n",
      "Walk\n",
      "Walking&party\n",
      "Watching TV\n",
      "Work\n",
      "Encoding: 0, Decoding: At home\n",
      "Encoding: 1, Decoding: Cooking\n",
      "Encoding: 2, Decoding: Eat\n",
      "Encoding: 3, Decoding: Football\n",
      "Encoding: 4, Decoding: In bus\n",
      "Encoding: 5, Decoding: In computer\n",
      "Encoding: 6, Decoding: In vehicle\n",
      "Encoding: 7, Decoding: Meeting\n",
      "Encoding: 8, Decoding: Movie\n",
      "Encoding: 9, Decoding: On bus stop\n",
      "Encoding: 10, Decoding: Pause\n",
      "Encoding: 11, Decoding: Phone was out of the pocket (forgot)\n",
      "Encoding: 12, Decoding: Picnic \n",
      "Encoding: 13, Decoding: Running\n",
      "Encoding: 14, Decoding: Shop\n",
      "Encoding: 15, Decoding: Shopping& wearing\n",
      "Encoding: 16, Decoding: Sleep\n",
      "Encoding: 17, Decoding: Took off glasses\n",
      "Encoding: 18, Decoding: Train\n",
      "Encoding: 19, Decoding: Video games\n",
      "Encoding: 20, Decoding: Walk\n",
      "Encoding: 21, Decoding: Walking&party\n",
      "Encoding: 22, Decoding: Watching TV\n",
      "Encoding: 23, Decoding: Work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4r/5qs7726s3_dc54vtgh1sc2740000gn/T/ipykernel_75090/3103381586.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pivot_data4_merged_filtered['activity_encoded'] = label_encoder_activities.fit_transform(pivot_data4_merged_filtered['activity_type'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create and fit the LabelEncoder\n",
    "label_encoder_activities = LabelEncoder()\n",
    "pivot_data4_merged_filtered['activity_encoded'] = label_encoder_activities.fit_transform(pivot_data4_merged_filtered['activity_type'])\n",
    "\n",
    "# Save the LabelEncoder to a file\n",
    "joblib.dump(label_encoder_activities, '/Users/emilkoch/Desktop/2Tango/messenger/Research/Job_Library/label_encoder_activities.pkl')\n",
    "\n",
    "# Get unique classes from the LabelEncoder\n",
    "unique_classes = label_encoder_activities.classes_\n",
    "\n",
    "# Print unique classes\n",
    "print(\"Unique Classes:\")\n",
    "for class_ in unique_classes:\n",
    "    print(class_)\n",
    "\n",
    "# Get the encodings and decodings\n",
    "encodings = label_encoder_activities.transform(label_encoder_activities.classes_)\n",
    "decodings = label_encoder_activities.classes_\n",
    "\n",
    "# Display the encodings and decodings\n",
    "for encoding, decoding in zip(encodings, decodings):\n",
    "    print(f\"Encoding: {encoding}, Decoding: {decoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4r/5qs7726s3_dc54vtgh1sc2740000gn/T/ipykernel_75090/179481233.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pivot_data4_merged_filtered['true_motion'] = pivot_data4_merged_filtered.apply(classify_motion_activity, axis=1)\n"
     ]
    }
   ],
   "source": [
    "def classify_motion_activity(row):\n",
    "    activity_encoded = row['activity_encoded']\n",
    "    if activity_encoded in [13, 20, 21, 3, 6, 18, 4]:\n",
    "        true_motion = 0\n",
    "    elif activity_encoded in [6, 18, 4, 13, 20, 21, 3]:\n",
    "        true_motion = 4\n",
    "    elif activity_encoded in [1, 7, 8, 15, 6, 18, 4, 13, 20, 21, 3]:\n",
    "        true_motion = 1\n",
    "    elif activity_encoded in [11, 17]:\n",
    "        true_motion = 3\n",
    "    elif activity_encoded in [0, 10, 14, 23, 2, 16, 22, 12, 19, 5, 9]:\n",
    "        true_motion = 2\n",
    "    return true_motion\n",
    "\n",
    "# Assuming 'pivot_data4' is your DataFrame\n",
    "pivot_data4_merged_filtered['true_motion'] = pivot_data4_merged_filtered.apply(classify_motion_activity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4r/5qs7726s3_dc54vtgh1sc2740000gn/T/ipykernel_75090/202448761.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pivot_data4_merged_filtered.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "pivot_data4_merged_filtered.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4r/5qs7726s3_dc54vtgh1sc2740000gn/T/ipykernel_75090/1887731920.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pivot_data4_merged_filtered['true_motion'] = pivot_data4_merged_filtered.apply(classify_motion_activity, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "246/246 [==============================] - 4s 6ms/step - loss: 3.0073 - accuracy: 0.1065 - val_loss: 2.3442 - val_accuracy: 0.1841\n",
      "Epoch 2/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 2.9543 - accuracy: 0.1402 - val_loss: 2.4476 - val_accuracy: 0.0575\n",
      "Epoch 3/20\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 2.6215 - accuracy: 0.1226 - val_loss: 2.1611 - val_accuracy: 0.2309\n",
      "Epoch 4/20\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 2.4224 - accuracy: 0.1304 - val_loss: 2.2138 - val_accuracy: 0.2274\n",
      "Epoch 5/20\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 2.2936 - accuracy: 0.1383 - val_loss: 2.0204 - val_accuracy: 0.1119\n",
      "Epoch 6/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 2.1969 - accuracy: 0.1443 - val_loss: 2.0008 - val_accuracy: 0.0946\n",
      "Epoch 7/20\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 2.0778 - accuracy: 0.1328 - val_loss: 1.9758 - val_accuracy: 0.1312\n",
      "Epoch 8/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.9430 - accuracy: 0.1331 - val_loss: 1.9860 - val_accuracy: 0.1068\n",
      "Epoch 9/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.9457 - accuracy: 0.1280 - val_loss: 1.8560 - val_accuracy: 0.1211\n",
      "Epoch 10/20\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 1.9424 - accuracy: 0.1340 - val_loss: 1.9065 - val_accuracy: 0.0916\n",
      "Epoch 11/20\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 1.8488 - accuracy: 0.1333 - val_loss: 1.9011 - val_accuracy: 0.0916\n",
      "Epoch 12/20\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 1.8418 - accuracy: 0.1316 - val_loss: 1.8749 - val_accuracy: 0.1317\n",
      "Epoch 13/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.6709 - accuracy: 0.1338 - val_loss: 1.8441 - val_accuracy: 0.1216\n",
      "Epoch 14/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.6654 - accuracy: 0.1351 - val_loss: 1.8720 - val_accuracy: 0.0916\n",
      "Epoch 15/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.6731 - accuracy: 0.1281 - val_loss: 1.8163 - val_accuracy: 0.1317\n",
      "Epoch 16/20\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.7357 - accuracy: 0.1225 - val_loss: 1.8369 - val_accuracy: 0.1455\n",
      "Epoch 17/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.6328 - accuracy: 0.1210 - val_loss: 1.8193 - val_accuracy: 0.1211\n",
      "Epoch 18/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.8311 - accuracy: 0.1355 - val_loss: 1.8079 - val_accuracy: 0.1317\n",
      "Epoch 19/20\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 1.5869 - accuracy: 0.1396 - val_loss: 1.7916 - val_accuracy: 0.0961\n",
      "Epoch 20/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.5764 - accuracy: 0.1260 - val_loss: 1.8030 - val_accuracy: 0.1144\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 1.7875 - accuracy: 0.1197\n",
      "Test Accuracy: 0.11965811997652054\n",
      "77/77 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.01      0.03       141\n",
      "           1       0.27      0.18      0.21        73\n",
      "           2       0.00      0.00      0.00       243\n",
      "           3       0.00      0.00      0.00        37\n",
      "           4       0.00      0.00      0.00       458\n",
      "           5       0.00      0.00      0.00       173\n",
      "           6       0.13      0.84      0.22       135\n",
      "           7       0.00      0.00      0.00       116\n",
      "           8       0.00      0.00      0.00       119\n",
      "           9       0.05      0.93      0.09        14\n",
      "          10       0.21      0.12      0.16       112\n",
      "          13       0.04      0.75      0.08        16\n",
      "          14       0.00      0.00      0.00        71\n",
      "          15       0.11      0.97      0.19        32\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.71      0.96      0.81        23\n",
      "          18       0.25      0.01      0.02       115\n",
      "          19       0.14      0.87      0.25        78\n",
      "          20       0.00      0.00      0.00       438\n",
      "          21       0.09      0.09      0.09        33\n",
      "          23       0.06      0.03      0.04        30\n",
      "\n",
      "    accuracy                           0.12      2457\n",
      "   macro avg       0.13      0.27      0.10      2457\n",
      "weighted avg       0.09      0.12      0.05      2457\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "# Define the classification function for motion activity\n",
    "def classify_motion_activity(row):\n",
    "    activity_encoded = row['activity_encoded']\n",
    "    if activity_encoded in [13, 20, 21, 3, 6, 18, 4]:\n",
    "        true_motion = 0\n",
    "    elif activity_encoded in [6, 18, 4, 13, 20, 21, 3]:\n",
    "        true_motion = 4\n",
    "    elif activity_encoded in [1, 7, 8, 15, 6, 18, 4, 13, 20, 21, 3]:\n",
    "        true_motion = 1\n",
    "    elif activity_encoded in [11, 17]:\n",
    "        true_motion = 3\n",
    "    elif activity_encoded in [0, 10, 14, 23, 2, 16, 22, 12, 19, 5, 9]:\n",
    "        true_motion = 2\n",
    "    return true_motion\n",
    "\n",
    "# Apply the function to create the 'true_motion' column\n",
    "pivot_data4_merged_filtered['true_motion'] = pivot_data4_merged_filtered.apply(classify_motion_activity, axis=1)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = pivot_data4_merged_filtered[['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z', 'true_motion', 'timestamp']]\n",
    "y = pivot_data4_merged_filtered['activity_encoded']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data for compatibility with CNN\n",
    "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(np.unique(pivot_data4_merged_filtered['activity_encoded'])), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Encode class labels for the new target variable\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "# Compute class weights using the encoded labels\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "\n",
    "# Create a class weight dictionary\n",
    "class_weight_dict = dict(zip(le.transform(le.classes_), class_weights))\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train, epochs=20, validation_split=0.2, class_weight=class_weight_dict, callbacks=[early_stopping])\n",
    "\n",
    "# Save the model\n",
    "file_path = \"/Users/emilkoch/Desktop/2Tango/messenger/research/Model_Save/activities.keras\"\n",
    "save_model(model, file_path)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test_reshaped)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "246/246 [==============================] - 5s 5ms/step - loss: 2.8335 - accuracy: 0.1233 - val_loss: 1.9436 - val_accuracy: 0.1338\n",
      "Epoch 2/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.7831 - accuracy: 0.2263 - val_loss: 1.3694 - val_accuracy: 0.1526\n",
      "Epoch 3/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.4615 - accuracy: 0.2398 - val_loss: 1.3089 - val_accuracy: 0.1414\n",
      "Epoch 4/20\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 1.3216 - accuracy: 0.2336 - val_loss: 1.2744 - val_accuracy: 0.1328\n",
      "Epoch 5/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.2889 - accuracy: 0.2034 - val_loss: 1.2493 - val_accuracy: 0.1368\n",
      "Epoch 6/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.2126 - accuracy: 0.2120 - val_loss: 1.2649 - val_accuracy: 0.1287\n",
      "Epoch 7/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.2127 - accuracy: 0.1941 - val_loss: 1.2610 - val_accuracy: 0.1267\n",
      "Epoch 8/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.2075 - accuracy: 0.1818 - val_loss: 1.2465 - val_accuracy: 0.1302\n",
      "Epoch 9/20\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.2272 - accuracy: 0.1691 - val_loss: 1.2800 - val_accuracy: 0.1241\n",
      "Epoch 10/20\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 1.2040 - accuracy: 0.1681 - val_loss: 1.2791 - val_accuracy: 0.1241\n",
      "Epoch 11/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.1699 - accuracy: 0.1538 - val_loss: 1.2984 - val_accuracy: 0.1190\n",
      "Epoch 12/20\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.1902 - accuracy: 0.1410 - val_loss: 1.3123 - val_accuracy: 0.1190\n",
      "Epoch 13/20\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.1856 - accuracy: 0.1536 - val_loss: 1.3216 - val_accuracy: 0.1226\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 1.3124 - accuracy: 0.1420\n",
      "Test Accuracy: 0.14204314351081848\n",
      "77/77 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.01      0.03      1232\n",
      "           1       0.17      0.89      0.29       340\n",
      "           2       0.33      0.01      0.02       862\n",
      "           3       0.03      0.83      0.06        23\n",
      "\n",
      "    accuracy                           0.14      2457\n",
      "   macro avg       0.27      0.44      0.10      2457\n",
      "weighted avg       0.41      0.14      0.06      2457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = pivot_data4_merged_filtered[['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z', 'timestamp']]\n",
    "y = pivot_data4_merged_filtered['true_motion']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(len(np.unique(pivot_data4_merged_filtered['activity_encoded'])), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Encode class labels for the new target variable\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "# Compute class weights using the encoded labels\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "\n",
    "# Create a class weight dictionary\n",
    "class_weight_dict = dict(zip(le.transform(le.classes_), class_weights))\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=20, validation_split=0.2, class_weight=class_weight_dict, callbacks=[early_stopping])\n",
    "\n",
    "# Save the model\n",
    "file_path = \"/Users/emilkoch/Desktop/2Tango/messenger/research/Model_Save/true_motion.keras\"\n",
    "save_model(model, file_path)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5427/5427 [==============================] - 12s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model_path = \"/Users/emilkoch/Desktop/2Tango/messenger/Research/Model_Save/true_motion.keras\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Preprocess the unlabeled data\n",
    "X_unlabeled_scaled = scaler.transform(pivot_data4[['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z', 'timestamp']])\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_unlabeled_scaled)\n",
    "\n",
    "# Assuming your model outputs probabilities and you want to select the class with the highest probability\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Add the predicted labels to the DataFrame\n",
    "pivot_data4['true_motion'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5427/5427 [==============================] - 11s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model_path = \"/Users/emilkoch/Desktop/2Tango/messenger/research/Model_Save/activities.keras\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Preprocess the unlabeled data\n",
    "X_unlabeled_scaled = scaler.transform(pivot_data4[['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z', 'true_motion', 'timestamp']])\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_unlabeled_scaled)\n",
    "\n",
    "# Assuming your model outputs probabilities and you want to select the class with the highest probability\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Add the predicted labels to the DataFrame\n",
    "pivot_data4['activity_encoded'] = y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
