{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from google.cloud import storage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('your_database.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute SQL query to retrieve data\n",
    "cursor.execute(\"SELECT motion, imagesign, tap, phonetic_complexity_data, phonetic_probability_data, iconizity, cluster_labels, gestureclass, Lemma_ID FROM UserData;\")\n",
    "data = cursor.fetchall()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Check if the fetched data is not empty\n",
    "if data:\n",
    "    # Extract individual columns from the fetched data\n",
    "    motion, imagesign, tap, phonetic_complexity_data, phonetic_probability_data, iconizity_data, cluster_labels, gestureclass, Lemma_ID = zip(*data)\n",
    "\n",
    "    # Ensure that the data are concatenated correctly for clustering\n",
    "    X = np.concatenate((motion, imagesign, tap, phonetic_complexity_data, phonetic_probability_data, iconizity_data, cluster_labels, gestureclass), axis=1)\n",
    "\n",
    "    # Standardize the numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Set X and y for further processing\n",
    "    X = X_scaled\n",
    "    y = Lemma_ID\n",
    "\n",
    "    # Split the data into training, validation, and testing sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Define your Keras model\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(len(np.unique(y)), activation='softmax')  # Use softmax activation for multi-class classification\n",
    "    ])\n",
    "\n",
    "    # Compile your model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',  # Use sparse categorical cross-entropy for integer labels\n",
    "                  metrics=['accuracy'])  # Use accuracy as a metric\n",
    "\n",
    "    # Train your model\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Evaluate your model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save('global_model_Lemma_ID.keras')\n",
    "\n",
    "    # Initialize a client\n",
    "    storage_client = storage.Client()  # Store model in Google Cloud Storage or any other cloud service\n",
    "\n",
    "    # Specify the bucket name and model file name\n",
    "    bucket_name = 'your_bucket_name'  # Ensure it doesn't start with a '/'\n",
    "    model_filename = 'global_model_Lemma_ID.keras'\n",
    "\n",
    "    # Upload the model file to the bucket\n",
    "    destination_blob_name = f'models/{model_filename}'  # Optional: Specify a folder path within the bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(model_filename)\n",
    "\n",
    "    # Make predictions for the entire dataset including probabilities\n",
    "    predictions_with_prob = model.predict(X)\n",
    "\n",
    "    # Connect to the database again to update the predictions\n",
    "    conn = sqlite3.connect('your_database.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Update the database with the predictions\n",
    "    for i, prediction in enumerate(predictions_with_prob):\n",
    "        # Extract the predicted class and its corresponding probability\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        probability = np.max(prediction)\n",
    "\n",
    "        # Update the database with the predicted class if probability is above the threshold, otherwise set to 'UNKNOWN'\n",
    "        if probability >= 0.6:\n",
    "            cursor.execute(\"UPDATE UserData SET Lemma_ID = ? WHERE rowid = ?\", (predicted_class, i+1))\n",
    "        else:\n",
    "            cursor.execute(\"UPDATE UserData SET Lemma_ID = ? WHERE rowid = ?\", ('UNKNOWN', i+1))\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "else:\n",
    "    pass  # Handle the case when there's no data fetched"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
