{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably a SQL relational data base is the best for our use-case where we want to have user data bases/tables and a global data base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from google.cloud import storage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('your_database.db')  # Replace 'your_database.db' with the actual database path\n",
    "\n",
    "# Execute SQL queries to retrieve data\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM MotionData;\")\n",
    "motion = cursor.fetchall()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM ImageData;\")\n",
    "imagesign = cursor.fetchall()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM tap;\")\n",
    "tap = cursor.fetchall()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM phonetic_complexity;\")\n",
    "phonetic_complexity_data  = cursor.fetchall()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM phonetic_probability;\")\n",
    "phonetic_probability_data = cursor.fetchall()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM iconizity;\")\n",
    "iconizity_data = cursor.fetchall()\n",
    "\n",
    "# Check if all data lists are empty\n",
    "if not (motion and imagesign and tap and phonetic_complexity_data and phonetic_probability_data and iconizity_data):\n",
    "    pass\n",
    "\n",
    "# Ensure that the data are concatenated correctly for clustering\n",
    "X = np.concatenate((motion, imagesign, tap, phonetic_complexity_data, phonetic_probability_data, iconizity_data), axis=1)\n",
    "\n",
    "# Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Calculate WCSS for different number of clusters\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X_scaled)  # Use scaled data for clustering\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Calculate the differences in WCSS\n",
    "differences = np.diff(wcss)\n",
    "\n",
    "# Find the elbow point (optimal number of clusters)\n",
    "elbow_point = np.argmax(differences) + 1\n",
    "\n",
    "# Initialize KMeans with optimal number of clusters\n",
    "n_clusters = elbow_point\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "\n",
    "# Fit KMeans to your scaled sensor data\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Execute SQL query to retrieve user IDs\n",
    "cursor.execute(\"SELECT user_id FROM UserData;\")\n",
    "user_ids = cursor.fetchall()\n",
    "\n",
    "# Iterate over user IDs and insert corresponding cluster labels\n",
    "for user_id, label in zip(user_ids, cluster_labels):\n",
    "    cursor.execute(\"INSERT INTO ClusterLabels (user_id, cluster_label) VALUES (?, ?)\", (user_id, label))\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, cluster_labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define your Keras model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(n_clusters, activation='softmax')  # Use softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Use sparse categorical cross-entropy for integer labels\n",
    "              metrics=['accuracy'])  # Use accuracy as a metric\n",
    "\n",
    "# Train your model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate your model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('global_model_clusters.keras')\n",
    "\n",
    "# Initialize a client for Google Cloud Storage - adjust depending on cloud provider used; \n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Specify the bucket name and model file name\n",
    "bucket_name = 'your_bucket_name'  # Replace 'your_bucket_name' with your actual bucket name\n",
    "model_filename = 'global_model_clusters.keras'\n",
    "\n",
    "# Upload the model file to the bucket\n",
    "destination_blob_name = f'models/{model_filename}'  # Optional: Specify a folder path within the bucket\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(destination_blob_name)\n",
    "blob.upload_from_filename(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a global model with all user data could be trained, saved, and uploaded to a cloud service for easy deployment. If no user data is available yet, clustering will be skipped. The elbow point calculation ensures that we use the optimal number of clusters and not an arbitrarily chosen pre-defined cluster number. Question: Standardize (one-hot) encoded categorical labels? A global model trained on all user data, cluster label tables created with user IDs in corresponding rows for easy retrieval. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
