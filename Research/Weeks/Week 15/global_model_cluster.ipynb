{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably a SQL relational data base is the best for our use-case where we want to have user data bases/tables and a global data base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from google.cloud import storage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('your_database.db')  # Replace 'your_database.db' with the actual database path\n",
    "\n",
    "# Execute SQL queries to retrieve data\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM MotionData;\")\n",
    "motion = cursor.fetchall()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM ImageData;\")\n",
    "imagesign = cursor.fetchall()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM tap;\")\n",
    "tap = cursor.fetchall()\n",
    "\n",
    "# Check if all data lists are empty\n",
    "if not (motion and imagesign and tap):\n",
    "    pass\n",
    "else:\n",
    "    # Perform one-hot encoding for tap data\n",
    "    if tap:\n",
    "        encoder = OneHotEncoder()\n",
    "        tap_encoded = encoder.fit_transform(tap).toarray()\n",
    "    else:\n",
    "        tap_encoded = None\n",
    "\n",
    "    # Combine motion, imagesign, and tap_encoded data\n",
    "    # Ensure that the data are concatenated correctly for clustering\n",
    "    X = np.concatenate((motion, imagesign, tap_encoded), axis=1)\n",
    "\n",
    "    # Standardize the numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Calculate WCSS for different number of clusters\n",
    "    wcss = []\n",
    "    for i in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        kmeans.fit(X_scaled)  # Use scaled data for clustering\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    # Calculate the differences in WCSS\n",
    "    differences = np.diff(wcss)\n",
    "\n",
    "    # Find the elbow point (optimal number of clusters)\n",
    "    elbow_point = np.argmax(differences) + 1\n",
    "\n",
    "    # Initialize KMeans with optimal number of clusters\n",
    "    n_clusters = elbow_point\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "\n",
    "    # Fit KMeans to your scaled sensor data\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "X = X_scaled\n",
    "y = cluster_labels\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define your Keras model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(n_clusters, activation='softmax')  # Use softmax activation for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Use sparse categorical cross-entropy for integer labels\n",
    "              metrics=['accuracy'])  # Use accuracy as a metric\n",
    "\n",
    "# Train your model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate your model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('global_model.keras')\n",
    "\n",
    "# Initialize a client\n",
    "storage_client = storage.Client() #store model in Google cloud or any other cloud service?\n",
    "\n",
    "# Specify the bucket name and model file name\n",
    "bucket_name = 'your_bucket_name'  # Ensure it doesn't start with a '/'\n",
    "model_filename = 'global_model.keras'\n",
    "\n",
    "# Upload the model file to the bucket\n",
    "destination_blob_name = f'models/{model_filename}'  # Optional: Specify a folder path within the bucket\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(destination_blob_name)\n",
    "blob.upload_from_filename(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a global model with all user data could be trained, saved, and uploaded to a cloud service for easy deployment. If no user data is available yet, clustering will be skipped. The elbow point calculation ensures that we use the optimal number of clusters and not an arbitrary chosen pre-defined cluster number. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
