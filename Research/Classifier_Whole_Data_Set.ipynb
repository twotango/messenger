{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 16:37:25.881788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "signdata = pd.read_csv('/Users/emilkoch/Library/Mobile Documents/com~apple~CloudDocs/Data Files/signdata.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns: ['List', 'Item', 'EnglishWF(lg10)', 'SignFrequency(M)', 'SignFrequency(SD)', 'SignFrequency(Z)', 'SignFrequency(N)', 'Unknown', 'SignFrequency(M-Native)', 'SignFrequency(SD-Native)', 'SignFreq(Z-native)', 'SignFrequency(N-Native)', 'Unknown(Native)', 'SignFrequency(M-Nonnative)', 'SignFrequency(SD-Nonnative)', 'SignFrequency(N-Nonnative)', 'SignFreq(Z-Nonnative)', 'Unknown(Nonnative)', 'DominantTranslationAgreement', 'DominantTranslationAgreement(Native)', 'DominantTranslationAgreement(Nonnative)', 'Iconicity(M)', 'Iconicity(SD)', 'Iconicity(Z)', 'Iconicity(N)', 'D.Iconicity(M)', 'D.Iconicity(SD)', 'D.Iconicity(N)', 'D.Iconicity(Z)', 'D.Iconicity(M-native)', 'D.Iconicity(SD-native)', 'D.Iconicity(Z-native)', 'D.Iconicity(N-native)', 'GuessConsistency', 'GuessAccuracy', 'Transparency(M)', 'Transparency SD', 'Transparency Z', 'Initialized.2.0', 'FingerspelledLoanSign.2.0', 'Compound.2.0', 'NumberOfMorphemes.2.0', 'SignOnset(ms)', 'SignOffset(ms)', 'SignDuration(ms)', 'ClipDuration(ms)', 'MarkedHandshape.2.0', 'FlexionChange.2.0', 'Spread.2.0', 'SpreadChange.2.0', 'ThumbContact.2.0', 'RepeatedMovement.2.0', 'Contact.2.0', 'UlnarRotation.2.0', 'MarkedHandshapeM2.2.0', 'FlexionChangeM2.2.0', 'SpreadM2.2.0', 'SpreadChangeM2.2.0', 'ThumbContactM2.2.0', 'RepeatedMovementM2.2.0', 'ContactM2.2.0', 'UlnarRotationM2.2.0', 'MarkedHandshapeM3.2.0', 'FlexionChangeM3.2.0', 'SpreadM3.2.0', 'SpreadChangeM3.2.0', 'ThumbContactM3.2.0', 'RepeatedMovementM3.2.0', 'ContactM3.2.0', 'UlnarRotationM3.2.0', 'MarkedHandshapeM4.2.0', 'FlexionChangeM4.2.0', 'SpreadM4.2.0', 'SpreadChangeM4.2.0', 'ThumbContactM4.2.0', 'RepeatedMovementM4.2.0', 'ContactM4.2.0', 'NonDominantHandshapeM4.2.0', 'UlnarRotationM4.2.0', 'MarkedHandshapeM5.2.0', 'FlexionChangeM5.2.0', 'SpreadM5.2.0', 'SpreadChangeM5.2.0', 'ThumbContactM5.2.0', 'SignTypeM5.2.0', 'MovementM5.2.0', 'RepeatedMovementM5.2.0', 'MajorLocationM5.2.0', 'MinorLocationM5.2.0', 'SecondMinorLocationM5.2.0', 'ContactM5.2.0', 'NonDominantHandshapeM5.2.0', 'UlnarRotationM5.2.0', 'MarkedHandshapeM6.2.0', 'FlexionChangeM6.2.0', 'SpreadM6.2.0', 'SpreadChangeM6.2.0', 'ThumbContactM6.2.0', 'SignTypeM6.2.0', 'MovementM6.2.0', 'RepeatedMovementM6.2.0', 'MajorLocationM6.2.0', 'MinorLocationM6.2.0', 'SecondMinorLocationM6.2.0', 'ContactM6.2.0', 'NonDominantHandshapeM6.2.0', 'UlnarRotationM6.2.0', 'SignType.2.0Frequency', 'MajorLocation.2.0Frequency', 'MinorLocation.2.0Frequency', 'SecondMinorLocation.2.0Frequency', 'Movement.2.0Frequency', 'SelectedFingers.2.0Frequency', 'Flexion.2.0Frequency', 'FlexionChange.2.0Frequency', 'RepeatedMovement.2.0Frequency', 'Contact.2.0Frequency', 'Spread.2.0Frequency', 'SpreadChange.2.0Frequency', 'ThumbContact.2.0Frequency', 'ThumbPosition.2.0Frequency', 'UlnarRotation.2.0Frequency', 'Neighborhood Density 2.0', 'Parameter.Neighborhood.Density.2.0', 'PhonotacticProbability', 'Phonological Complexity', 'SignBankReferenceID', 'bglm_aoa', 'empirical_aoa']\n",
      "Categorical Columns: ['EntryID', 'LemmaID', 'Code', 'Batch', 'DominantTranslation', 'NondominantTranslations', 'Iconicity_ID', 'IconicityType', 'LexicalClass', 'Handshape.2.0', 'SelectedFingers.2.0', 'Flexion.2.0', 'ThumbPosition.2.0', 'SignType.2.0', 'Movement.2.0', 'MajorLocation.2.0', 'MinorLocation.2.0', 'SecondMinorLocation.2.0', 'NonDominantHandshape.2.0', 'HandshapeM2.2.0', 'SelectedFingersM2.2.0', 'FlexionM2.2.0', 'ThumbPositionM2.2.0', 'SignTypeM2.2.0', 'MovementM2.2.0', 'MajorLocationM2.2.0', 'MinorLocationM2.2.0', 'SecondMinorLocationM2.2.0', 'NonDominantHandshapeM2.2.0', 'HandshapeM3.2.0', 'SelectedFingersM3.2.0', 'FlexionM3.2.0', 'ThumbPositionM3.2.0', 'SignTypeM3.2.0', 'MovementM3.2.0', 'MajorLocationM3.2.0', 'MinorLocationM3.2.0', 'SecondMinorLocationM3.2.0', 'NonDominantHandshapeM3.2.0', 'HandshapeM4.2.0', 'SelectedFingersM4.2.0', 'FlexionM4.2.0', 'ThumbPositionM4.2.0', 'SignTypeM4.2.0', 'MovementM4.2.0', 'MajorLocationM4.2.0', 'MinorLocationM4.2.0', 'SecondMinorLocationM4.2.0', 'HandshapeM5.2.0', 'SelectedFingersM5.2.0', 'FlexionM5.2.0', 'ThumbPositionM5.2.0', 'HandshapeM6.2.0', 'SelectedFingersM6.2.0', 'FlexionM6.2.0', 'ThumbPositionM6.2.0', 'SignBankAnnotationID', 'SignBankLemmaID', 'SignBankSemanticField', 'InCDI', 'CDISemanticCategory']\n"
     ]
    }
   ],
   "source": [
    "# Separate target variable from features\n",
    "X = signdata.drop(columns=['SignBankEnglishTranslations'])  # Features\n",
    "y = signdata['SignBankEnglishTranslations'].astype(str)\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2723\n",
      "129\n",
      "   List  Item  EnglishWF(lg10)  SignFrequency(M)  SignFrequency(SD)  \\\n",
      "0     1     2            3.521             5.143              2.081   \n",
      "1     1     3            4.645             6.032              1.516   \n",
      "2     1     4            2.600             4.429              1.720   \n",
      "3     1     5            2.928             2.621              1.720   \n",
      "4     1     8            3.041             1.579              0.838   \n",
      "\n",
      "   SignFrequency(Z)  SignFrequency(N)  Unknown  SignFrequency(M-Native)  \\\n",
      "0             0.621                21    0.000                    5.167   \n",
      "1             1.068                31    0.000                    6.111   \n",
      "2             0.232                21    0.000                    4.167   \n",
      "3            -0.753                29    0.065                    2.000   \n",
      "4            -1.198                19    0.095                    1.455   \n",
      "\n",
      "   SignFrequency(SD-Native)  ...  ThumbContact.2.0Frequency  \\\n",
      "0                     2.167  ...                      0.684   \n",
      "1                     1.568  ...                      0.684   \n",
      "2                     1.899  ...                      0.684   \n",
      "3                     1.317  ...                      0.316   \n",
      "4                     0.688  ...                      0.684   \n",
      "\n",
      "   ThumbPosition.2.0Frequency  UlnarRotation.2.0Frequency  \\\n",
      "0                       0.657                       0.164   \n",
      "1                       0.657                       0.836   \n",
      "2                       0.657                       0.836   \n",
      "3                       0.343                       0.164   \n",
      "4                       0.343                       0.836   \n",
      "\n",
      "   Neighborhood Density 2.0  Parameter.Neighborhood.Density.2.0  \\\n",
      "0                         4                                 190   \n",
      "1                         5                                 391   \n",
      "2                        11                                 488   \n",
      "3                         0                                 220   \n",
      "4                         1                                 453   \n",
      "\n",
      "   PhonotacticProbability  Phonological Complexity  SignBankReferenceID  \\\n",
      "0                   0.147                      1.0                342.0   \n",
      "1                   0.099                      1.0                199.0   \n",
      "2                   0.821                      2.0               1844.0   \n",
      "3                  -0.505                      2.0               3011.0   \n",
      "4                   0.226                      2.0               2471.0   \n",
      "\n",
      "   bglm_aoa  empirical_aoa  \n",
      "0      22.0           14.0  \n",
      "1      31.0           18.0  \n",
      "2      32.0           28.0  \n",
      "3       NaN            NaN  \n",
      "4       NaN            NaN  \n",
      "\n",
      "[5 rows x 129 columns]\n",
      "List                          0\n",
      "Item                          0\n",
      "EnglishWF(lg10)             334\n",
      "SignFrequency(M)              0\n",
      "SignFrequency(SD)             0\n",
      "                           ... \n",
      "PhonotacticProbability        0\n",
      "Phonological Complexity      26\n",
      "SignBankReferenceID         734\n",
      "bglm_aoa                   2190\n",
      "empirical_aoa              2190\n",
      "Length: 129, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for numerical features\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Copy numerical columns\n",
    "X_numerical = X[numerical_cols].copy()\n",
    "print(len(X_numerical))\n",
    "print(len(numerical_cols))\n",
    "print(X_numerical.head())\n",
    "print(X_numerical.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['UlnarRotationM4.2.0' 'FlexionChangeM5.2.0' 'SpreadChangeM5.2.0'\n",
      " 'SignTypeM5.2.0' 'MovementM5.2.0' 'RepeatedMovementM5.2.0'\n",
      " 'MajorLocationM5.2.0' 'MinorLocationM5.2.0' 'SecondMinorLocationM5.2.0'\n",
      " 'ContactM5.2.0' 'NonDominantHandshapeM5.2.0' 'UlnarRotationM5.2.0'\n",
      " 'FlexionChangeM6.2.0' 'SpreadChangeM6.2.0' 'SignTypeM6.2.0'\n",
      " 'MovementM6.2.0' 'RepeatedMovementM6.2.0' 'MajorLocationM6.2.0'\n",
      " 'MinorLocationM6.2.0' 'SecondMinorLocationM6.2.0' 'ContactM6.2.0'\n",
      " 'NonDominantHandshapeM6.2.0' 'UlnarRotationM6.2.0']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values and scaling\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_numerical_imputed = imputer.fit_transform(X_numerical) \n",
    "scaler = StandardScaler()\n",
    "X_numerical_scaled  = scaler.fit_transform(X_numerical_imputed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for categorical features\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# Copy categorical columns\n",
    "X_categorical = X[categorical_cols].copy()\n",
    "\n",
    "# Impute missing values in categorical columns\n",
    "X_categorical = pd.DataFrame(categorical_imputer.fit_transform(X_categorical), columns=X_categorical.columns)\n",
    "\n",
    "# Encode categorical features\n",
    "encoded_cols = pd.DataFrame(encoder.fit_transform(X_categorical))\n",
    "encoded_cols.columns = encoder.get_feature_names_out(categorical_cols)\n",
    "categorical_cols_encoded = encoded_cols.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate numerical and encoded categorical columns\n",
    "X_processed = pd.concat([pd.DataFrame(X_numerical_scaled), encoded_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "69/69 [==============================] - 6s 58ms/step - loss: 0.5071 - val_loss: 0.2547 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.5032 - val_loss: 0.2526 - lr: 9.0484e-04\n",
      "Epoch 3/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.4992 - val_loss: 0.2502 - lr: 8.1873e-04\n",
      "Epoch 4/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.4937 - val_loss: 0.2466 - lr: 7.4082e-04\n",
      "Epoch 5/50\n",
      "69/69 [==============================] - 4s 55ms/step - loss: 0.4863 - val_loss: 0.2417 - lr: 6.7032e-04\n",
      "Epoch 6/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.4758 - val_loss: 0.2358 - lr: 6.0653e-04\n",
      "Epoch 7/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.4624 - val_loss: 0.2286 - lr: 5.4881e-04\n",
      "Epoch 8/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.4463 - val_loss: 0.2200 - lr: 4.9659e-04\n",
      "Epoch 9/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.4282 - val_loss: 0.2107 - lr: 4.4933e-04\n",
      "Epoch 10/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.4091 - val_loss: 0.2017 - lr: 4.0657e-04\n",
      "Epoch 11/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.3892 - val_loss: 0.1922 - lr: 3.6788e-04\n",
      "Epoch 12/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.3696 - val_loss: 0.1825 - lr: 3.3287e-04\n",
      "Epoch 13/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.3506 - val_loss: 0.1733 - lr: 3.0119e-04\n",
      "Epoch 14/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.3324 - val_loss: 0.1646 - lr: 2.7253e-04\n",
      "Epoch 15/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.3155 - val_loss: 0.1561 - lr: 2.4660e-04\n",
      "Epoch 16/50\n",
      "69/69 [==============================] - 4s 50ms/step - loss: 0.2999 - val_loss: 0.1491 - lr: 2.2313e-04\n",
      "Epoch 17/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.2857 - val_loss: 0.1424 - lr: 2.0190e-04\n",
      "Epoch 18/50\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.2727 - val_loss: 0.1365 - lr: 1.8268e-04\n",
      "Epoch 19/50\n",
      "69/69 [==============================] - 4s 56ms/step - loss: 0.2610 - val_loss: 0.1312 - lr: 1.6530e-04\n",
      "Epoch 20/50\n",
      " 2/69 [..............................] - ETA: 10s - loss: 0.2541"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the decreasing learning rate schedule\n",
    "def decreasing_schedule(epoch):\n",
    "    return 0.001 * np.exp(-0.1 * epoch)\n",
    "\n",
    "def train_autoencoder(X_train, X_test):\n",
    "    # Define the autoencoder architecture\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = 64  # Adjust as needed\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder_layer1 = Dense(128, activation='relu')(input_layer)\n",
    "    encoder_layer1 = BatchNormalization()(encoder_layer1)\n",
    "    encoder_layer1 = Dropout(0.5)(encoder_layer1)\n",
    "\n",
    "    encoder_layer2 = Dense(encoding_dim, activation='relu')(encoder_layer1)\n",
    "    encoder_layer2 = BatchNormalization()(encoder_layer2)\n",
    "    encoder_layer2 = Dropout(0.5)(encoder_layer2)\n",
    "\n",
    "    decoder_layer1 = Dense(128, activation='relu')(encoder_layer2)\n",
    "    decoder_layer1 = BatchNormalization()(decoder_layer1)\n",
    "\n",
    "    decoder_layer2 = Dense(input_dim, activation='sigmoid')(decoder_layer1)  # Adjusted output dimensionality\n",
    "    decoder_layer2 = Dropout(0.5)(decoder_layer2)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoder_layer2)\n",
    "\n",
    "    # Define the optimizer with RMSprop\n",
    "    optimizer = RMSprop(learning_rate=0.001) \n",
    "\n",
    "    # Compile the autoencoder model with RMSprop optimizer\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Define the learning rate scheduler callback\n",
    "    lr_scheduler = LearningRateScheduler(decreasing_schedule)\n",
    "\n",
    "    # Train the autoencoder with learning rate scheduler\n",
    "    autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, \n",
    "                    validation_data=(X_test, X_test), callbacks=[lr_scheduler])\n",
    "\n",
    "    # Extract features using the encoder part of the autoencoder\n",
    "    encoder = Model(input_layer, encoder_layer2)\n",
    "    X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "    # Reconstruct data using the trained autoencoder\n",
    "    reconstructed_data = autoencoder.predict(X_test)\n",
    "\n",
    "    # Combine original test data with reconstructed data\n",
    "    X_test_combined = np.concatenate((X_test, reconstructed_data), axis=1)\n",
    "\n",
    "    # Compute cosine similarity between original and reconstructed data samples\n",
    "    cosine_similarities = cosine_similarity(X_test_combined)\n",
    "\n",
    "    # Calculate the mean cosine similarity across all samples\n",
    "    mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "    \n",
    "    return mean_cosine_similarity, encoder_layer2, input_layer\n",
    "\n",
    "# Assuming you have your data stored in X_processed and y\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the autoencoder with the decreasing learning rate schedule and RMSprop optimizer\n",
    "similarity, encoder_layer2, input_layer = train_autoencoder(X_train, X_test)\n",
    "print(\"Mean Cosine Similarity:\", similarity)\n",
    "\n",
    "# Extract features using the encoder part of the autoencoder\n",
    "encoder = Model(input_layer, encoder_layer2)\n",
    "X_encoded_train = encoder.predict(X_train)\n",
    "X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the categorical labels\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_classifier = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier.fit(X_encoded_train, y_train_encoded)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_encoded = xgb_classifier.predict(X_encoded_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train: (2178,)\n",
      "Expected shape of y_train for prediction: (1461,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of y_train\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "# Check the expected shape of the target variable for prediction\n",
    "print(\"Expected shape of y_train for prediction:\", xgb_classifier.classes_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1984, 1803)\n"
     ]
    }
   ],
   "source": [
    "print(y_one_hot.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
