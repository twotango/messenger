{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 16:37:25.881788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "signdata = pd.read_csv('/Users/emilkoch/Library/Mobile Documents/com~apple~CloudDocs/Data Files/signdata.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns: ['List', 'Item', 'EnglishWF(lg10)', 'SignFrequency(M)', 'SignFrequency(SD)', 'SignFrequency(Z)', 'SignFrequency(N)', 'Unknown', 'SignFrequency(M-Native)', 'SignFrequency(SD-Native)', 'SignFreq(Z-native)', 'SignFrequency(N-Native)', 'Unknown(Native)', 'SignFrequency(M-Nonnative)', 'SignFrequency(SD-Nonnative)', 'SignFrequency(N-Nonnative)', 'SignFreq(Z-Nonnative)', 'Unknown(Nonnative)', 'DominantTranslationAgreement', 'DominantTranslationAgreement(Native)', 'DominantTranslationAgreement(Nonnative)', 'Iconicity(M)', 'Iconicity(SD)', 'Iconicity(Z)', 'Iconicity(N)', 'D.Iconicity(M)', 'D.Iconicity(SD)', 'D.Iconicity(N)', 'D.Iconicity(Z)', 'D.Iconicity(M-native)', 'D.Iconicity(SD-native)', 'D.Iconicity(Z-native)', 'D.Iconicity(N-native)', 'GuessConsistency', 'GuessAccuracy', 'Transparency(M)', 'Transparency SD', 'Transparency Z', 'Initialized.2.0', 'FingerspelledLoanSign.2.0', 'Compound.2.0', 'NumberOfMorphemes.2.0', 'SignOnset(ms)', 'SignOffset(ms)', 'SignDuration(ms)', 'ClipDuration(ms)', 'MarkedHandshape.2.0', 'FlexionChange.2.0', 'Spread.2.0', 'SpreadChange.2.0', 'ThumbContact.2.0', 'RepeatedMovement.2.0', 'Contact.2.0', 'UlnarRotation.2.0', 'MarkedHandshapeM2.2.0', 'FlexionChangeM2.2.0', 'SpreadM2.2.0', 'SpreadChangeM2.2.0', 'ThumbContactM2.2.0', 'RepeatedMovementM2.2.0', 'ContactM2.2.0', 'UlnarRotationM2.2.0', 'MarkedHandshapeM3.2.0', 'FlexionChangeM3.2.0', 'SpreadM3.2.0', 'SpreadChangeM3.2.0', 'ThumbContactM3.2.0', 'RepeatedMovementM3.2.0', 'ContactM3.2.0', 'UlnarRotationM3.2.0', 'MarkedHandshapeM4.2.0', 'FlexionChangeM4.2.0', 'SpreadM4.2.0', 'SpreadChangeM4.2.0', 'ThumbContactM4.2.0', 'RepeatedMovementM4.2.0', 'ContactM4.2.0', 'NonDominantHandshapeM4.2.0', 'UlnarRotationM4.2.0', 'MarkedHandshapeM5.2.0', 'FlexionChangeM5.2.0', 'SpreadM5.2.0', 'SpreadChangeM5.2.0', 'ThumbContactM5.2.0', 'SignTypeM5.2.0', 'MovementM5.2.0', 'RepeatedMovementM5.2.0', 'MajorLocationM5.2.0', 'MinorLocationM5.2.0', 'SecondMinorLocationM5.2.0', 'ContactM5.2.0', 'NonDominantHandshapeM5.2.0', 'UlnarRotationM5.2.0', 'MarkedHandshapeM6.2.0', 'FlexionChangeM6.2.0', 'SpreadM6.2.0', 'SpreadChangeM6.2.0', 'ThumbContactM6.2.0', 'SignTypeM6.2.0', 'MovementM6.2.0', 'RepeatedMovementM6.2.0', 'MajorLocationM6.2.0', 'MinorLocationM6.2.0', 'SecondMinorLocationM6.2.0', 'ContactM6.2.0', 'NonDominantHandshapeM6.2.0', 'UlnarRotationM6.2.0', 'SignType.2.0Frequency', 'MajorLocation.2.0Frequency', 'MinorLocation.2.0Frequency', 'SecondMinorLocation.2.0Frequency', 'Movement.2.0Frequency', 'SelectedFingers.2.0Frequency', 'Flexion.2.0Frequency', 'FlexionChange.2.0Frequency', 'RepeatedMovement.2.0Frequency', 'Contact.2.0Frequency', 'Spread.2.0Frequency', 'SpreadChange.2.0Frequency', 'ThumbContact.2.0Frequency', 'ThumbPosition.2.0Frequency', 'UlnarRotation.2.0Frequency', 'Neighborhood Density 2.0', 'Parameter.Neighborhood.Density.2.0', 'PhonotacticProbability', 'Phonological Complexity', 'SignBankReferenceID', 'bglm_aoa', 'empirical_aoa']\n",
      "Categorical Columns: ['EntryID', 'LemmaID', 'Code', 'Batch', 'DominantTranslation', 'NondominantTranslations', 'Iconicity_ID', 'IconicityType', 'LexicalClass', 'Handshape.2.0', 'SelectedFingers.2.0', 'Flexion.2.0', 'ThumbPosition.2.0', 'SignType.2.0', 'Movement.2.0', 'MajorLocation.2.0', 'MinorLocation.2.0', 'SecondMinorLocation.2.0', 'NonDominantHandshape.2.0', 'HandshapeM2.2.0', 'SelectedFingersM2.2.0', 'FlexionM2.2.0', 'ThumbPositionM2.2.0', 'SignTypeM2.2.0', 'MovementM2.2.0', 'MajorLocationM2.2.0', 'MinorLocationM2.2.0', 'SecondMinorLocationM2.2.0', 'NonDominantHandshapeM2.2.0', 'HandshapeM3.2.0', 'SelectedFingersM3.2.0', 'FlexionM3.2.0', 'ThumbPositionM3.2.0', 'SignTypeM3.2.0', 'MovementM3.2.0', 'MajorLocationM3.2.0', 'MinorLocationM3.2.0', 'SecondMinorLocationM3.2.0', 'NonDominantHandshapeM3.2.0', 'HandshapeM4.2.0', 'SelectedFingersM4.2.0', 'FlexionM4.2.0', 'ThumbPositionM4.2.0', 'SignTypeM4.2.0', 'MovementM4.2.0', 'MajorLocationM4.2.0', 'MinorLocationM4.2.0', 'SecondMinorLocationM4.2.0', 'HandshapeM5.2.0', 'SelectedFingersM5.2.0', 'FlexionM5.2.0', 'ThumbPositionM5.2.0', 'HandshapeM6.2.0', 'SelectedFingersM6.2.0', 'FlexionM6.2.0', 'ThumbPositionM6.2.0', 'SignBankAnnotationID', 'SignBankLemmaID', 'SignBankSemanticField', 'InCDI', 'CDISemanticCategory']\n"
     ]
    }
   ],
   "source": [
    "# Separate target variable from features\n",
    "X = signdata.drop(columns=['SignBankEnglishTranslations'])  # Features\n",
    "y = signdata['SignBankEnglishTranslations']\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2723\n",
      "129\n",
      "   List  Item  EnglishWF(lg10)  SignFrequency(M)  SignFrequency(SD)  \\\n",
      "0     1     2            3.521             5.143              2.081   \n",
      "1     1     3            4.645             6.032              1.516   \n",
      "2     1     4            2.600             4.429              1.720   \n",
      "3     1     5            2.928             2.621              1.720   \n",
      "4     1     8            3.041             1.579              0.838   \n",
      "\n",
      "   SignFrequency(Z)  SignFrequency(N)  Unknown  SignFrequency(M-Native)  \\\n",
      "0             0.621                21    0.000                    5.167   \n",
      "1             1.068                31    0.000                    6.111   \n",
      "2             0.232                21    0.000                    4.167   \n",
      "3            -0.753                29    0.065                    2.000   \n",
      "4            -1.198                19    0.095                    1.455   \n",
      "\n",
      "   SignFrequency(SD-Native)  ...  ThumbContact.2.0Frequency  \\\n",
      "0                     2.167  ...                      0.684   \n",
      "1                     1.568  ...                      0.684   \n",
      "2                     1.899  ...                      0.684   \n",
      "3                     1.317  ...                      0.316   \n",
      "4                     0.688  ...                      0.684   \n",
      "\n",
      "   ThumbPosition.2.0Frequency  UlnarRotation.2.0Frequency  \\\n",
      "0                       0.657                       0.164   \n",
      "1                       0.657                       0.836   \n",
      "2                       0.657                       0.836   \n",
      "3                       0.343                       0.164   \n",
      "4                       0.343                       0.836   \n",
      "\n",
      "   Neighborhood Density 2.0  Parameter.Neighborhood.Density.2.0  \\\n",
      "0                         4                                 190   \n",
      "1                         5                                 391   \n",
      "2                        11                                 488   \n",
      "3                         0                                 220   \n",
      "4                         1                                 453   \n",
      "\n",
      "   PhonotacticProbability  Phonological Complexity  SignBankReferenceID  \\\n",
      "0                   0.147                      1.0                342.0   \n",
      "1                   0.099                      1.0                199.0   \n",
      "2                   0.821                      2.0               1844.0   \n",
      "3                  -0.505                      2.0               3011.0   \n",
      "4                   0.226                      2.0               2471.0   \n",
      "\n",
      "   bglm_aoa  empirical_aoa  \n",
      "0      22.0           14.0  \n",
      "1      31.0           18.0  \n",
      "2      32.0           28.0  \n",
      "3       NaN            NaN  \n",
      "4       NaN            NaN  \n",
      "\n",
      "[5 rows x 129 columns]\n",
      "List                          0\n",
      "Item                          0\n",
      "EnglishWF(lg10)             334\n",
      "SignFrequency(M)              0\n",
      "SignFrequency(SD)             0\n",
      "                           ... \n",
      "PhonotacticProbability        0\n",
      "Phonological Complexity      26\n",
      "SignBankReferenceID         734\n",
      "bglm_aoa                   2190\n",
      "empirical_aoa              2190\n",
      "Length: 129, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for numerical features\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Copy numerical columns\n",
    "X_numerical = X[numerical_cols].copy()\n",
    "print(len(X_numerical))\n",
    "print(len(numerical_cols))\n",
    "print(X_numerical.head())\n",
    "print(X_numerical.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['UlnarRotationM4.2.0' 'FlexionChangeM5.2.0' 'SpreadChangeM5.2.0'\n",
      " 'SignTypeM5.2.0' 'MovementM5.2.0' 'RepeatedMovementM5.2.0'\n",
      " 'MajorLocationM5.2.0' 'MinorLocationM5.2.0' 'SecondMinorLocationM5.2.0'\n",
      " 'ContactM5.2.0' 'NonDominantHandshapeM5.2.0' 'UlnarRotationM5.2.0'\n",
      " 'FlexionChangeM6.2.0' 'SpreadChangeM6.2.0' 'SignTypeM6.2.0'\n",
      " 'MovementM6.2.0' 'RepeatedMovementM6.2.0' 'MajorLocationM6.2.0'\n",
      " 'MinorLocationM6.2.0' 'SecondMinorLocationM6.2.0' 'ContactM6.2.0'\n",
      " 'NonDominantHandshapeM6.2.0' 'UlnarRotationM6.2.0']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values and scaling\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_numerical_imputed = imputer.fit_transform(X_numerical) \n",
    "scaler = StandardScaler()\n",
    "X_numerical_scaled  = scaler.fit_transform(X_numerical_imputed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for categorical features\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# Copy categorical columns\n",
    "X_categorical = X[categorical_cols].copy()\n",
    "\n",
    "# Impute missing values in categorical columns\n",
    "X_categorical = pd.DataFrame(categorical_imputer.fit_transform(X_categorical), columns=X_categorical.columns)\n",
    "\n",
    "# Encode categorical features\n",
    "encoded_cols = pd.DataFrame(encoder.fit_transform(X_categorical))\n",
    "encoded_cols.columns = encoder.get_feature_names_out(categorical_cols)\n",
    "categorical_cols_encoded = encoded_cols.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate numerical and encoded categorical columns\n",
    "X_processed = pd.concat([pd.DataFrame(X_numerical_scaled), encoded_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'SignBankEnglishTranslations' column: 739\n",
      "Number of NaN values in 'SignBankEnglishTranslations' column after imputation: 0\n",
      "<U183\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in the target variable\n",
    "nan_count = signdata['SignBankEnglishTranslations'].isnull().sum()\n",
    "print(\"Number of NaN values in 'SignBankEnglishTranslations' column:\", nan_count)\n",
    "\n",
    "# Initialize SimpleImputer\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Apply SimpleImputer to fill missing values in the target variable\n",
    "filled_values = imputer.fit_transform(signdata[['SignBankEnglishTranslations']])\n",
    "y_imputed = filled_values.flatten().astype(str)  # Flatten the 2D array to 1D before assigning back to the Series\n",
    "\n",
    "# Check for NaN values in the target variable after imputation\n",
    "nan_count_after_impute = pd.Series(y_imputed).isnull().sum()\n",
    "print(\"Number of NaN values in 'SignBankEnglishTranslations' column after imputation:\", nan_count_after_impute)\n",
    "print(y_imputed.dtype)\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to the target variable 'SignBankEnglishTranslations'\n",
    "y_encoded = label_encoder.fit_transform(y_imputed)\n",
    "print(y_encoded.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: '520'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '520'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply label encoding to the target variable 'SignBankEnglishTranslations' for y_train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_train_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Set y_train to have the same classes as y_encoded\u001b[39;00m\n\u001b[1;32m      5\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train_encoded\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: '520'"
     ]
    }
   ],
   "source": [
    "# Apply label encoding to the target variable 'SignBankEnglishTranslations' for y_train\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "\n",
    "# Set y_train to have the same classes as y_encoded\n",
    "y_train = y_train_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "69/69 [==============================] - 7s 64ms/step - loss: 0.5072 - val_loss: 0.2547\n",
      "Epoch 2/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.5030 - val_loss: 0.2524\n",
      "Epoch 3/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.4980 - val_loss: 0.2490\n",
      "Epoch 4/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.4900 - val_loss: 0.2433\n",
      "Epoch 5/50\n",
      "69/69 [==============================] - 3s 51ms/step - loss: 0.4752 - val_loss: 0.2327\n",
      "Epoch 6/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.4487 - val_loss: 0.2160\n",
      "Epoch 7/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.4072 - val_loss: 0.1911\n",
      "Epoch 8/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.3510 - val_loss: 0.1598\n",
      "Epoch 9/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.2854 - val_loss: 0.1257\n",
      "Epoch 10/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.2194 - val_loss: 0.0942\n",
      "Epoch 11/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.1611 - val_loss: 0.0678\n",
      "Epoch 12/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.1152 - val_loss: 0.0493\n",
      "Epoch 13/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.0818 - val_loss: 0.0361\n",
      "Epoch 14/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0586 - val_loss: 0.0272\n",
      "Epoch 15/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.0430 - val_loss: 0.0212\n",
      "Epoch 16/50\n",
      "69/69 [==============================] - 4s 57ms/step - loss: 0.0326 - val_loss: 0.0173\n",
      "Epoch 17/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.0258 - val_loss: 0.0147\n",
      "Epoch 18/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0212 - val_loss: 0.0130\n",
      "Epoch 19/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0182 - val_loss: 0.0119\n",
      "Epoch 20/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0161 - val_loss: 0.0111\n",
      "Epoch 21/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.0147 - val_loss: 0.0106\n",
      "Epoch 22/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0137 - val_loss: 0.0102\n",
      "Epoch 23/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.0130 - val_loss: 0.0099\n",
      "Epoch 24/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0125 - val_loss: 0.0097\n",
      "Epoch 25/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.0121 - val_loss: 0.0095\n",
      "Epoch 26/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0118 - val_loss: 0.0094\n",
      "Epoch 27/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.0115 - val_loss: 0.0093\n",
      "Epoch 28/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0113 - val_loss: 0.0092\n",
      "Epoch 29/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0111 - val_loss: 0.0091\n",
      "Epoch 30/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.0110 - val_loss: 0.0090\n",
      "Epoch 31/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0109 - val_loss: 0.0090\n",
      "Epoch 32/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0107 - val_loss: 0.0089\n",
      "Epoch 33/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.0107 - val_loss: 0.0089\n",
      "Epoch 34/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0106 - val_loss: 0.0088\n",
      "Epoch 35/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0105 - val_loss: 0.0088\n",
      "Epoch 36/50\n",
      "69/69 [==============================] - 4s 54ms/step - loss: 0.0104 - val_loss: 0.0088\n",
      "Epoch 37/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0104 - val_loss: 0.0087\n",
      "Epoch 38/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.0103 - val_loss: 0.0087\n",
      "Epoch 39/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.0103 - val_loss: 0.0087\n",
      "Epoch 40/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0102 - val_loss: 0.0087\n",
      "Epoch 41/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0102 - val_loss: 0.0086\n",
      "Epoch 42/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0101 - val_loss: 0.0086\n",
      "Epoch 43/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0101 - val_loss: 0.0086\n",
      "Epoch 44/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0101 - val_loss: 0.0086\n",
      "Epoch 45/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0101 - val_loss: 0.0086\n",
      "Epoch 46/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0100 - val_loss: 0.0086\n",
      "Epoch 47/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0100 - val_loss: 0.0085\n",
      "Epoch 48/50\n",
      "69/69 [==============================] - 3s 51ms/step - loss: 0.0100 - val_loss: 0.0085\n",
      "Epoch 49/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0100 - val_loss: 0.0085\n",
      "Epoch 50/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0099 - val_loss: 0.0085\n",
      "69/69 [==============================] - 0s 5ms/step\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "18/18 [==============================] - 0s 9ms/step\n",
      "Mean Cosine Similarity: 0.3918759372627433\n"
     ]
    }
   ],
   "source": [
    "# Define the autoencoder architecture with increased complexity, regularization, and batch normalization\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 64  # Adjust as needed\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "# Add a dense layer with ReLU activation and batch normalization\n",
    "encoder_layer1 = Dense(128, activation='relu')(input_layer)\n",
    "encoder_layer1 = BatchNormalization()(encoder_layer1)\n",
    "# Add a dropout layer for regularization\n",
    "encoder_layer1 = Dropout(0.5)(encoder_layer1)\n",
    "\n",
    "# Add another dense layer with ReLU activation and batch normalization\n",
    "encoder_layer2 = Dense(encoding_dim, activation='relu')(encoder_layer1)\n",
    "encoder_layer2 = BatchNormalization()(encoder_layer2)\n",
    "# Add a dropout layer for regularization\n",
    "encoder_layer2 = Dropout(0.5)(encoder_layer2)\n",
    "\n",
    "decoder_layer1 = Dense(128, activation='relu')(encoder_layer2)\n",
    "decoder_layer1 = BatchNormalization()(decoder_layer1)\n",
    "\n",
    "decoder_layer2 = Dense(input_dim, activation='sigmoid')(decoder_layer1)\n",
    "# Add a dropout layer for regularization\n",
    "decoder_layer2 = Dropout(0.5)(decoder_layer2)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(input_layer, decoder_layer2)\n",
    "\n",
    "# Compile the autoencoder model with RMSprop optimizer\n",
    "autoencoder.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, validation_data=(X_test, X_test))\n",
    "\n",
    "# Extract features using the encoder part of the autoencoder\n",
    "encoder = Model(input_layer, encoder_layer2)\n",
    "X_encoded_train = encoder.predict(X_train)\n",
    "X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "# Reconstruct data using the trained autoencoder\n",
    "reconstructed_data = autoencoder.predict(X_test)\n",
    "\n",
    "# Combine original test data with reconstructed data\n",
    "X_test_combined = np.concatenate((X_test, reconstructed_data), axis=1)\n",
    "\n",
    "# Compute cosine similarity between original and reconstructed data samples\n",
    "cosine_similarities = cosine_similarity(X_test_combined)\n",
    "\n",
    "# Calculate the mean cosine similarity across all samples\n",
    "mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "print(\"Mean Cosine Similarity:\", mean_cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate schedule: constant\n",
      "Epoch 1/50\n",
      "69/69 [==============================] - 7s 62ms/step - loss: 0.5072 - val_loss: 0.2547 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "69/69 [==============================] - 3s 44ms/step - loss: 0.5030 - val_loss: 0.2525 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "69/69 [==============================] - 4s 57ms/step - loss: 0.4980 - val_loss: 0.2493 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "69/69 [==============================] - 3s 44ms/step - loss: 0.4900 - val_loss: 0.2436 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.4750 - val_loss: 0.2326 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "69/69 [==============================] - 4s 57ms/step - loss: 0.4482 - val_loss: 0.2149 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.4065 - val_loss: 0.1898 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "69/69 [==============================] - 3s 42ms/step - loss: 0.3502 - val_loss: 0.1586 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.2845 - val_loss: 0.1253 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.2184 - val_loss: 0.0940 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "69/69 [==============================] - 5s 73ms/step - loss: 0.1602 - val_loss: 0.0686 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "69/69 [==============================] - 4s 58ms/step - loss: 0.1143 - val_loss: 0.0493 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.0811 - val_loss: 0.0358 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "69/69 [==============================] - 3s 45ms/step - loss: 0.0581 - val_loss: 0.0270 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "69/69 [==============================] - 3s 51ms/step - loss: 0.0426 - val_loss: 0.0210 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0323 - val_loss: 0.0172 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.0256 - val_loss: 0.0147 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.0210 - val_loss: 0.0130 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "69/69 [==============================] - 3s 45ms/step - loss: 0.0180 - val_loss: 0.0119 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "69/69 [==============================] - 4s 61ms/step - loss: 0.0160 - val_loss: 0.0111 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "69/69 [==============================] - 3s 45ms/step - loss: 0.0147 - val_loss: 0.0106 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "69/69 [==============================] - 3s 44ms/step - loss: 0.0137 - val_loss: 0.0102 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0130 - val_loss: 0.0099 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.0125 - val_loss: 0.0097 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.0121 - val_loss: 0.0095 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "69/69 [==============================] - 4s 54ms/step - loss: 0.0117 - val_loss: 0.0094 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "69/69 [==============================] - 4s 61ms/step - loss: 0.0115 - val_loss: 0.0093 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "69/69 [==============================] - 4s 62ms/step - loss: 0.0113 - val_loss: 0.0092 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "69/69 [==============================] - 5s 68ms/step - loss: 0.0111 - val_loss: 0.0091 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.0110 - val_loss: 0.0090 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "69/69 [==============================] - 4s 62ms/step - loss: 0.0108 - val_loss: 0.0090 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "69/69 [==============================] - 6s 89ms/step - loss: 0.0107 - val_loss: 0.0089 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "69/69 [==============================] - 5s 73ms/step - loss: 0.0106 - val_loss: 0.0089 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "69/69 [==============================] - 3s 51ms/step - loss: 0.0106 - val_loss: 0.0088 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "69/69 [==============================] - 5s 65ms/step - loss: 0.0105 - val_loss: 0.0088 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "69/69 [==============================] - 3s 41ms/step - loss: 0.0104 - val_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "69/69 [==============================] - 3s 43ms/step - loss: 0.0104 - val_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0103 - val_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.0103 - val_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "69/69 [==============================] - 4s 55ms/step - loss: 0.0102 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0102 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "69/69 [==============================] - 5s 67ms/step - loss: 0.0101 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0101 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0101 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "69/69 [==============================] - 4s 58ms/step - loss: 0.0100 - val_loss: 0.0085 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "69/69 [==============================] - 4s 55ms/step - loss: 0.0100 - val_loss: 0.0085 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "69/69 [==============================] - 4s 61ms/step - loss: 0.0100 - val_loss: 0.0085 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.0100 - val_loss: 0.0085 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.0099 - val_loss: 0.0085 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "69/69 [==============================] - 4s 55ms/step - loss: 0.0099 - val_loss: 0.0085 - lr: 0.0010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m schedule_name, schedule_func \u001b[38;5;129;01min\u001b[39;00m schedules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with learning rate schedule:\u001b[39m\u001b[38;5;124m\"\u001b[39m, schedule_name)\n\u001b[0;32m---> 80\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschedule_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Cosine Similarity:\u001b[39m\u001b[38;5;124m\"\u001b[39m, similarity)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m best_similarity:\n",
      "Cell \u001b[0;32mIn[18], line 50\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(X_train, X_test, schedule)\u001b[0m\n\u001b[1;32m     47\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m LearningRateScheduler(schedule)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Train the autoencoder with learning rate scheduler\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Extract features using the encoder part of the autoencoder\u001b[39;00m\n\u001b[1;32m     54\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Model(input_layer, encoder_layer2)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "\n",
    "# Define the learning rate schedules\n",
    "schedules = {\n",
    "    'constant': lambda epoch: 0.001,\n",
    "    'decreasing': lambda epoch: 0.001 * np.exp(-0.1 * epoch),\n",
    "    'adaptive': lambda epoch: 0.001 if epoch < 10 else 0.0005\n",
    "}\n",
    "\n",
    "# Define a function to train the autoencoder with a given learning rate schedule\n",
    "def train_autoencoder(X_train, X_test, schedule):\n",
    "    # Define the autoencoder architecture\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = 64  # Adjust as needed\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder_layer1 = Dense(128, activation='relu')(input_layer)\n",
    "    encoder_layer1 = BatchNormalization()(encoder_layer1)\n",
    "    encoder_layer1 = Dropout(0.5)(encoder_layer1)\n",
    "\n",
    "    encoder_layer2 = Dense(encoding_dim, activation='relu')(encoder_layer1)\n",
    "    encoder_layer2 = BatchNormalization()(encoder_layer2)\n",
    "    encoder_layer2 = Dropout(0.5)(encoder_layer2)\n",
    "\n",
    "    decoder_layer1 = Dense(128, activation='relu')(encoder_layer2)\n",
    "    decoder_layer1 = BatchNormalization()(decoder_layer1)\n",
    "\n",
    "    decoder_layer2 = Dense(input_dim, activation='sigmoid')(decoder_layer1)\n",
    "    decoder_layer2 = Dropout(0.5)(decoder_layer2)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoder_layer2)\n",
    "\n",
    "    # Define the optimizer with RMSprop\n",
    "    optimizer = RMSprop(learning_rate=0.001) \n",
    "\n",
    "    # Compile the autoencoder model with RMSprop optimizer\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Define the learning rate scheduler callback\n",
    "    lr_scheduler = LearningRateScheduler(schedule)\n",
    "\n",
    "    # Train the autoencoder with learning rate scheduler\n",
    "    autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, \n",
    "                    validation_data=(X_test, X_test), callbacks=[lr_scheduler])\n",
    "\n",
    "    # Extract features using the encoder part of the autoencoder\n",
    "    encoder = Model(input_layer, encoder_layer2)\n",
    "    X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "    # Reconstruct data using the trained autoencoder\n",
    "    reconstructed_data = autoencoder.predict(X_test)\n",
    "\n",
    "    # Combine original test data with reconstructed data\n",
    "    X_test_combined = np.concatenate((X_test, reconstructed_data), axis=1)\n",
    "\n",
    "    # Compute cosine similarity between original and reconstructed data samples\n",
    "    cosine_similarities = cosine_similarity(X_test_combined)\n",
    "\n",
    "    # Calculate the mean cosine similarity across all samples\n",
    "    mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "    \n",
    "    return mean_cosine_similarity\n",
    "\n",
    "# Assuming you have your data stored in X_processed and y_encoded\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the autoencoder with different learning rate schedules and keep track of the best one\n",
    "best_schedule = None\n",
    "best_similarity = -1\n",
    "for schedule_name, schedule_func in schedules.items():\n",
    "    print(\"Training with learning rate schedule:\", schedule_name)\n",
    "    similarity = train_autoencoder(X_train, X_test, schedule_func)\n",
    "    print(\"Mean Cosine Similarity:\", similarity)\n",
    "    if similarity > best_similarity:\n",
    "        best_similarity = similarity\n",
    "        best_schedule = schedule_name\n",
    "\n",
    "print(\"Best learning rate schedule:\", best_schedule)\n",
    "print(\"Best Mean Cosine Similarity:\", best_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "69/69 [==============================] - 6s 56ms/step - loss: 0.5071 - val_loss: 0.2547 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.5034 - val_loss: 0.2527 - lr: 9.0484e-04\n",
      "Epoch 3/50\n",
      "69/69 [==============================] - 3s 45ms/step - loss: 0.4990 - val_loss: 0.2501 - lr: 8.1873e-04\n",
      "Epoch 4/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.4938 - val_loss: 0.2463 - lr: 7.4082e-04\n",
      "Epoch 5/50\n",
      "69/69 [==============================] - 3s 45ms/step - loss: 0.4862 - val_loss: 0.2409 - lr: 6.7032e-04\n",
      "Epoch 6/50\n",
      "69/69 [==============================] - 3s 45ms/step - loss: 0.4758 - val_loss: 0.2342 - lr: 6.0653e-04\n",
      "Epoch 7/50\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.4626 - val_loss: 0.2258 - lr: 5.4881e-04\n",
      "Epoch 8/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.4468 - val_loss: 0.2171 - lr: 4.9659e-04\n",
      "Epoch 9/50\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.4289 - val_loss: 0.2079 - lr: 4.4933e-04\n",
      "Epoch 10/50\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.4100 - val_loss: 0.1983 - lr: 4.0657e-04\n",
      "Epoch 11/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.3905 - val_loss: 0.1890 - lr: 3.6788e-04\n",
      "Epoch 12/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.3709 - val_loss: 0.1783 - lr: 3.3287e-04\n",
      "Epoch 13/50\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.3520 - val_loss: 0.1685 - lr: 3.0119e-04\n",
      "Epoch 14/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.3339 - val_loss: 0.1598 - lr: 2.7253e-04\n",
      "Epoch 15/50\n",
      "69/69 [==============================] - 3s 45ms/step - loss: 0.3173 - val_loss: 0.1514 - lr: 2.4660e-04\n",
      "Epoch 16/50\n",
      "69/69 [==============================] - 4s 56ms/step - loss: 0.3017 - val_loss: 0.1434 - lr: 2.2313e-04\n",
      "Epoch 17/50\n",
      "69/69 [==============================] - 5s 67ms/step - loss: 0.2875 - val_loss: 0.1366 - lr: 2.0190e-04\n",
      "Epoch 18/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.2747 - val_loss: 0.1305 - lr: 1.8268e-04\n",
      "Epoch 19/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.2630 - val_loss: 0.1250 - lr: 1.6530e-04\n",
      "Epoch 20/50\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.2525 - val_loss: 0.1204 - lr: 1.4957e-04\n",
      "Epoch 21/50\n",
      "69/69 [==============================] - 4s 58ms/step - loss: 0.2431 - val_loss: 0.1160 - lr: 1.3534e-04\n",
      "Epoch 22/50\n",
      "69/69 [==============================] - 5s 67ms/step - loss: 0.2346 - val_loss: 0.1118 - lr: 1.2246e-04\n",
      "Epoch 23/50\n",
      "69/69 [==============================] - 4s 57ms/step - loss: 0.2271 - val_loss: 0.1081 - lr: 1.1080e-04\n",
      "Epoch 24/50\n",
      "69/69 [==============================] - 4s 58ms/step - loss: 0.2205 - val_loss: 0.1050 - lr: 1.0026e-04\n",
      "Epoch 25/50\n",
      "69/69 [==============================] - 4s 57ms/step - loss: 0.2144 - val_loss: 0.1023 - lr: 9.0718e-05\n",
      "Epoch 26/50\n",
      "69/69 [==============================] - 5s 69ms/step - loss: 0.2090 - val_loss: 0.0998 - lr: 8.2085e-05\n",
      "Epoch 27/50\n",
      "69/69 [==============================] - 4s 64ms/step - loss: 0.2042 - val_loss: 0.0975 - lr: 7.4274e-05\n",
      "Epoch 28/50\n",
      "69/69 [==============================] - 4s 61ms/step - loss: 0.1998 - val_loss: 0.0955 - lr: 6.7206e-05\n",
      "Epoch 29/50\n",
      "69/69 [==============================] - 4s 54ms/step - loss: 0.1960 - val_loss: 0.0936 - lr: 6.0810e-05\n",
      "Epoch 30/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.1925 - val_loss: 0.0921 - lr: 5.5023e-05\n",
      "Epoch 31/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.1894 - val_loss: 0.0908 - lr: 4.9787e-05\n",
      "Epoch 32/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.1866 - val_loss: 0.0894 - lr: 4.5049e-05\n",
      "Epoch 33/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.1841 - val_loss: 0.0882 - lr: 4.0762e-05\n",
      "Epoch 34/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.1819 - val_loss: 0.0872 - lr: 3.6883e-05\n",
      "Epoch 35/50\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.1799 - val_loss: 0.0862 - lr: 3.3373e-05\n",
      "Epoch 36/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.1781 - val_loss: 0.0855 - lr: 3.0197e-05\n",
      "Epoch 37/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.1764 - val_loss: 0.0847 - lr: 2.7324e-05\n",
      "Epoch 38/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.1751 - val_loss: 0.0840 - lr: 2.4724e-05\n",
      "Epoch 39/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.1737 - val_loss: 0.0834 - lr: 2.2371e-05\n",
      "Epoch 40/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.1725 - val_loss: 0.0830 - lr: 2.0242e-05\n",
      "Epoch 41/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.1715 - val_loss: 0.0824 - lr: 1.8316e-05\n",
      "Epoch 42/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.1705 - val_loss: 0.0819 - lr: 1.6573e-05\n",
      "Epoch 43/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.1696 - val_loss: 0.0815 - lr: 1.4996e-05\n",
      "Epoch 44/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.1689 - val_loss: 0.0811 - lr: 1.3569e-05\n",
      "Epoch 45/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.1681 - val_loss: 0.0808 - lr: 1.2277e-05\n",
      "Epoch 46/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.1675 - val_loss: 0.0806 - lr: 1.1109e-05\n",
      "Epoch 47/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.1670 - val_loss: 0.0803 - lr: 1.0052e-05\n",
      "Epoch 48/50\n",
      "69/69 [==============================] - 3s 51ms/step - loss: 0.1664 - val_loss: 0.0799 - lr: 9.0953e-06\n",
      "Epoch 49/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.1659 - val_loss: 0.0798 - lr: 8.2297e-06\n",
      "Epoch 50/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.1656 - val_loss: 0.0797 - lr: 7.4466e-06\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "18/18 [==============================] - 0s 9ms/step\n",
      "Mean Cosine Similarity: 0.907591150066248\n",
      "69/69 [==============================] - 1s 5ms/step\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Shape of X_encoded_train: (2178, 64)\n",
      "Shape of X_encoded_test: (545, 64)\n",
      "Sample values of X_encoded_train:\n",
      "[[ 2.36996579e+00  1.48755622e+00 -6.72714591e-01 -6.00752592e-01\n",
      "   2.64363241e+00 -5.78965485e-01 -4.02676314e-01 -6.19660079e-01\n",
      "   1.11584103e+00 -5.65579593e-01 -2.77314156e-01  1.30928826e+00\n",
      "  -4.48746979e-01  2.96678960e-01 -2.48752266e-01 -5.21172166e-01\n",
      "   1.43754172e+00 -6.27220273e-02  1.77748203e-01 -5.51538765e-01\n",
      "  -3.72498006e-01  1.03093481e+00 -6.40473127e-01  4.49701548e-02\n",
      "  -4.76691604e-01  1.63021684e-01 -5.13691902e-01 -5.52273333e-01\n",
      "  -9.21751559e-02 -6.96613550e-01 -3.55103731e-01 -6.11601055e-01\n",
      "  -5.84715426e-01 -2.77208596e-01  4.67738628e-01  4.46971476e-01\n",
      "  -7.17880666e-01 -6.91395342e-01  2.01406574e+00  2.03121662e+00\n",
      "  -6.43178701e-01 -5.54091692e-01 -5.51593423e-01  9.94626105e-01\n",
      "   1.34673667e+00 -3.44062626e-01 -5.98843694e-01  2.02238202e+00\n",
      "  -5.94369113e-01 -3.40497792e-01 -6.45203948e-01 -5.84935129e-01\n",
      "  -5.09218097e-01 -5.67894220e-01  2.20268941e+00 -5.19244730e-01\n",
      "  -1.20505482e-01 -4.09907967e-01 -1.31388456e-01  3.23082209e-02\n",
      "  -5.10664582e-01 -7.63146818e-01 -4.14712727e-01  6.84701800e-01]\n",
      " [ 2.82103300e+00  3.89688540e+00 -6.72714591e-01 -6.00752592e-01\n",
      "  -6.21793330e-01 -4.39959526e-01 -6.64823830e-01 -6.19660079e-01\n",
      "   1.14310563e+00 -5.65579593e-01  2.15671349e+00  2.77341795e+00\n",
      "   1.13750267e+00  1.04565382e-01 -5.09265900e-01  1.31206286e+00\n",
      "   2.34861076e-01 -5.46568274e-01  1.12035131e+00 -5.51538765e-01\n",
      "   8.64000142e-01  2.74745750e+00 -6.40473127e-01  4.55768287e-01\n",
      "  -4.76691604e-01  2.35931695e-01  8.10397267e-02 -5.52273333e-01\n",
      "  -5.71244955e-01 -6.96613550e-01 -5.26349783e-01 -6.11601055e-01\n",
      "   9.90820944e-01  3.44929361e+00  2.21005034e+00 -5.57607710e-01\n",
      "  -7.17880666e-01 -6.91395342e-01  6.60546958e-01  2.91474962e+00\n",
      "  -6.43178701e-01 -5.54091692e-01 -5.51593423e-01  6.97343349e-02\n",
      "   2.62302828e+00 -2.41611093e-01 -5.46292543e-01  2.20702481e+00\n",
      "  -5.94369113e-01 -4.95280772e-01 -6.45203948e-01 -5.84935129e-01\n",
      "   3.08328331e-01  4.82245803e-01  2.18584967e+00 -4.92262930e-01\n",
      "   2.76275682e+00 -3.34441185e-01 -4.79948699e-01  1.00477672e+00\n",
      "   8.65074277e-01 -7.63146818e-01  2.29088283e+00  8.80353093e-01]\n",
      " [ 1.18194246e+00  1.07808912e+00 -6.72714591e-01 -6.00752592e-01\n",
      "   2.35413313e-01 -8.84634256e-03 -6.64823830e-01 -6.19660079e-01\n",
      "  -1.71089590e-01 -5.65579593e-01  8.33463252e-01  4.71144855e-01\n",
      "  -4.48746979e-01 -5.81502318e-02 -5.09265900e-01 -5.71079731e-01\n",
      "  -5.01745045e-01 -5.46568274e-01  3.36257875e-01 -5.51538765e-01\n",
      "  -3.82344604e-01  6.38658762e-01 -6.40473127e-01  3.90699089e-01\n",
      "  -4.76691604e-01 -9.59104896e-02  3.91880691e-01 -5.52273333e-01\n",
      "  -5.71244955e-01 -6.96613550e-01 -5.26349783e-01 -3.48899305e-01\n",
      "  -1.97765529e-01  1.14560854e+00  9.04630899e-01 -5.57607710e-01\n",
      "  -7.17880666e-01 -6.91395342e-01  9.79946911e-01  1.14989543e+00\n",
      "  -6.43178701e-01 -5.54091692e-01 -5.51593423e-01 -4.87899244e-01\n",
      "  -5.07810295e-01 -3.07499200e-01 -5.98843694e-01  1.10853744e+00\n",
      "  -5.94369113e-01 -4.95280772e-01 -1.46101415e-01 -5.84935129e-01\n",
      "   4.26166654e-01  4.18185532e-01  1.19061339e+00 -5.19244730e-01\n",
      "   3.77268076e-01 -4.61827725e-01 -6.17797375e-01 -3.95307660e-01\n",
      "  -5.10664582e-01 -7.63146818e-01  6.95974648e-01 -5.77135801e-01]\n",
      " [-3.60858440e-03  8.42688799e-01 -6.72714591e-01 -6.00752592e-01\n",
      "  -6.51627779e-02 -1.18059754e-01 -6.64823830e-01  8.10899556e-01\n",
      "  -5.68119884e-01 -5.65579593e-01  7.62635469e-03 -1.51657134e-01\n",
      "  -4.48746979e-01 -1.58525676e-01 -4.85751361e-01 -2.95163006e-01\n",
      "  -5.01745045e-01 -5.46568274e-01 -4.49571073e-01  3.61653030e-01\n",
      "  -4.63740170e-01  7.64602900e-01 -6.40473127e-01 -3.13855916e-01\n",
      "  -4.76691604e-01 -5.89194238e-01 -5.13691902e-01 -5.52273333e-01\n",
      "  -5.71244955e-01 -1.05310619e-01 -5.26349783e-01 -2.56368101e-01\n",
      "  -2.95707971e-01  1.47412205e+00  7.54589558e-01 -5.57607710e-01\n",
      "  -7.17880666e-01 -6.91395342e-01 -5.39897621e-01 -2.60224223e-01\n",
      "   3.16723406e-01 -5.54091692e-01 -5.51593423e-01 -4.87899244e-01\n",
      "  -5.07810295e-01 -4.71652567e-01 -5.98843694e-01 -2.82619894e-01\n",
      "  -5.77182174e-01 -4.95280772e-01 -6.45203948e-01 -5.84935129e-01\n",
      "  -5.09218097e-01 -4.01176125e-01 -5.86061478e-01 -1.98741138e-01\n",
      "   1.71133578e-01 -4.80528951e-01 -6.17797375e-01 -4.47623491e-01\n",
      "  -5.10664582e-01 -7.63146818e-01  1.26638222e+00 -4.41024780e-01]\n",
      " [-6.07948482e-01 -6.10953331e-01 -5.63651323e-02  8.60781789e-01\n",
      "   9.03210700e-01  5.47665358e-03 -6.64823830e-01 -5.13748229e-01\n",
      "   5.41020632e-02  3.29059899e-01 -2.06950128e-01 -5.55932105e-01\n",
      "  -4.48746979e-01 -5.14236867e-01  1.17942989e+00 -5.71079731e-01\n",
      "  -5.01745045e-01  1.31241715e+00 -5.59611380e-01 -1.11342639e-01\n",
      "  -4.63740170e-01 -6.16626740e-01  1.84959888e-01 -6.29596412e-01\n",
      "  -2.05054611e-01  3.84149134e-01 -5.13691902e-01 -3.96026492e-01\n",
      "  -5.71244955e-01 -5.71984768e-01  4.45927382e-01 -3.31462204e-01\n",
      "   4.62713063e-01 -5.96637249e-01 -6.18390322e-01  1.38622928e+00\n",
      "   5.61452329e-01 -6.91395342e-01 -5.39897621e-01  1.15856552e+00\n",
      "  -5.17400503e-02 -2.63037950e-01 -5.51593423e-01 -4.87899244e-01\n",
      "  -5.07810295e-01 -3.18197906e-01 -5.98843694e-01  5.10154307e-01\n",
      "  -1.18719131e-01  5.89475989e-01  6.71387315e-02  1.97665715e+00\n",
      "   1.40940487e-01 -4.20446783e-01 -5.86061478e-01 -5.19244730e-01\n",
      "   2.26577044e-01 -4.80528951e-01  1.77709293e+00 -5.30096829e-01\n",
      "  -5.10664582e-01  5.00484407e-01 -5.86412370e-01 -5.77135801e-01]]\n",
      "Sample values of X_encoded_test:\n",
      "[[ 1.0006878   2.7775712  -0.6727146  -0.6007526  -0.6217933   0.9128637\n",
      "  -0.66482383 -0.6196601   0.8058609  -0.5655796   1.6633577   2.8048067\n",
      "   0.5253306   1.9213266  -0.5092659   1.2087246  -0.26626706 -0.5465683\n",
      "   2.1779583  -0.55153877 -0.46374017  0.5864258  -0.6404731  -0.4069777\n",
      "  -0.4766916   0.4084859   0.23413211 -0.55227333 -0.57124496 -0.69661355\n",
      "   0.3286835  -0.61160105 -0.25561762  1.3346279   1.6698909  -0.5576077\n",
      "  -0.71788067 -0.69139534  1.1188221   0.36985624 -0.54451495 -0.5540917\n",
      "  -0.5515934  -0.16660509  1.1578236   0.85535556 -0.45873448  1.0658383\n",
      "  -0.5943691  -0.49528077 -0.64520395 -0.5849351   1.275416    1.8411374\n",
      "   0.44628    -0.51924473  1.0448763  -0.07180366 -0.6177974   1.6649544\n",
      "   0.8826419  -0.7631468   2.859599    1.5427532 ]\n",
      " [-0.6079485  -0.61095333 -0.6727146  -0.5893845  -0.1484305  -0.5789655\n",
      "  -0.56188184  0.3860671  -0.5681199   0.24945056 -0.61982924 -0.5559321\n",
      "  -0.44874698 -0.51423687  0.03529006 -0.57107973 -0.50174505 -0.2875058\n",
      "  -0.5596114   0.17018956 -0.46374017 -0.09764218 -0.6404731  -0.6295964\n",
      "  -0.43574616 -0.58919424 -0.5136919  -0.55227333 -0.57124496 -0.40726045\n",
      "  -0.5263498  -0.61160105  0.6763486  -0.6497474  -0.6183903   0.0595113\n",
      "  -0.28566164 -0.244804   -0.5398976  -0.62475234 -0.6251489  -0.5540917\n",
      "   0.2817191  -0.48789924 -0.5078103  -0.28153592 -0.5988437  -0.62064165\n",
      "   0.20325679 -0.49528077 -0.64520395  0.21919852 -0.5092181  -0.5678942\n",
      "  -0.5860615  -0.21638265 -0.6041373  -0.48052895 -0.61208385 -0.5300968\n",
      "  -0.5106646   0.5023739  -0.5864124  -0.5771358 ]\n",
      " [-0.6079485  -0.61095333  0.87918615  0.8574251  -0.11004359 -0.5789655\n",
      "   0.4953882  -0.2220521  -0.5681199  -0.18101901 -0.61982924 -0.5559321\n",
      "  -0.44874698 -0.31229353 -0.5092659  -0.57107973 -0.50174505 -0.5465683\n",
      "  -0.5596114  -0.55153877 -0.46374017 -0.61662674  0.13317788 -0.5803763\n",
      "  -0.4766916  -0.17621255 -0.3559779  -0.55227333  1.0755831  -0.69661355\n",
      "  -0.5263498  -0.61160105 -0.5847154  -0.6497474  -0.6183903  -0.5576077\n",
      "   0.12361121  0.01034904 -0.5398976  -0.62475234 -0.6431787   0.21526247\n",
      "   0.40765756 -0.48789924 -0.5078103  -0.43058172 -0.5988437  -0.62064165\n",
      "  -0.5943691  -0.29483607 -0.64520395 -0.45729202 -0.5092181  -0.5678942\n",
      "  -0.51439065  0.11472714 -0.6041373  -0.48052895 -0.6177974  -0.5300968\n",
      "  -0.5106646   0.6987367  -0.5864124  -0.5771358 ]\n",
      " [-0.6079485  -0.2539967  -0.6727146  -0.49308202 -0.2555119  -0.5789655\n",
      "  -0.66482383 -0.6196601  -0.5681199  -0.5655796  -0.61982924 -0.45363835\n",
      "  -0.44874698 -0.51423687 -0.5092659  -0.57107973 -0.50174505 -0.5465683\n",
      "  -0.493075   -0.55153877 -0.46374017 -0.61662674  0.05192947 -0.6295964\n",
      "  -0.4766916   0.03223813 -0.20677438 -0.55227333 -0.57124496  2.146095\n",
      "   0.26127088 -0.09907651 -0.5847154  -0.5755683  -0.6183903  -0.24864566\n",
      "  -0.71788067 -0.69139534 -0.5398976  -0.62475234 -0.6431787  -0.5540917\n",
      "  -0.5515934  -0.48789924 -0.5078103  -0.47165257 -0.5988437  -0.62064165\n",
      "  -0.5943691  -0.49528077  1.3350521  -0.5849351   0.78768575 -0.28765753\n",
      "  -0.5860615  -0.51924473 -0.47717583 -0.48052895 -0.46414348 -0.5300968\n",
      "  -0.37508243 -0.40861937 -0.5864124  -0.5771358 ]\n",
      " [-0.6079485  -0.25469998 -0.6727146  -0.12721244 -0.6217933  -0.33728728\n",
      "  -0.66482383 -0.44663495 -0.5681199  -0.5655796  -0.61982924 -0.5559321\n",
      "  -0.44874698 -0.51423687 -0.5092659  -0.38544935 -0.50174505 -0.5465683\n",
      "  -0.5596114  -0.55153877 -0.46374017 -0.61662674 -0.4198885  -0.6295964\n",
      "  -0.4766916  -0.22403198 -0.45600957 -0.55227333 -0.57124496  0.07269114\n",
      "  -0.36340606 -0.61160105 -0.05871409 -0.3548383  -0.6183903  -0.09381258\n",
      "  -0.69880426 -0.69139534 -0.5398976  -0.23976758  0.07771522 -0.5540917\n",
      "  -0.5515934  -0.48789924 -0.34145045 -0.47165257 -0.5988437  -0.62064165\n",
      "  -0.4967788  -0.49528077  1.451079   -0.52165836  0.19153959 -0.01614642\n",
      "  -0.5860615  -0.51924473 -0.25686532 -0.48052895  0.527295   -0.5300968\n",
      "  -0.43079948 -0.56694406 -0.5864124  -0.5771358 ]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [   0    1    2 ... 1458 1459 1460], got [   1    3    5 ... 1800 1801 1802]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m xgb_classifier \u001b[38;5;241m=\u001b[39m XGBClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[43mxgb_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_encoded_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     97\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m xgb_classifier\u001b[38;5;241m.\u001b[39mpredict(X_encoded_test)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/sklearn.py:1471\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1468\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m   1470\u001b[0m ):\n\u001b[0;32m-> 1471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1472\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1473\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1474\u001b[0m     )\n\u001b[1;32m   1476\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [   0    1    2 ... 1458 1459 1460], got [   1    3    5 ... 1800 1801 1802]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Define the decreasing learning rate schedule\n",
    "def decreasing_schedule(epoch):\n",
    "    return 0.001 * np.exp(-0.1 * epoch)\n",
    "\n",
    "def train_autoencoder(X_train, X_test):\n",
    "    # Define the autoencoder architecture\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = 64  # Adjust as needed\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder_layer1 = Dense(128, activation='relu')(input_layer)\n",
    "    encoder_layer1 = BatchNormalization()(encoder_layer1)\n",
    "    encoder_layer1 = Dropout(0.5)(encoder_layer1)\n",
    "\n",
    "    encoder_layer2 = Dense(encoding_dim, activation='relu')(encoder_layer1)\n",
    "    encoder_layer2 = BatchNormalization()(encoder_layer2)\n",
    "    encoder_layer2 = Dropout(0.5)(encoder_layer2)\n",
    "\n",
    "    decoder_layer1 = Dense(128, activation='relu')(encoder_layer2)\n",
    "    decoder_layer1 = BatchNormalization()(decoder_layer1)\n",
    "\n",
    "    decoder_layer2 = Dense(input_dim, activation='sigmoid')(decoder_layer1)  # Adjusted output dimensionality\n",
    "    decoder_layer2 = Dropout(0.5)(decoder_layer2)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoder_layer2)\n",
    "\n",
    "    # Define the optimizer with RMSprop\n",
    "    optimizer = RMSprop(learning_rate=0.001) \n",
    "\n",
    "    # Compile the autoencoder model with RMSprop optimizer\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Define the learning rate scheduler callback\n",
    "    lr_scheduler = LearningRateScheduler(decreasing_schedule)\n",
    "\n",
    "    # Train the autoencoder with learning rate scheduler\n",
    "    autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, \n",
    "                    validation_data=(X_test, X_test), callbacks=[lr_scheduler])\n",
    "\n",
    "    # Extract features using the encoder part of the autoencoder\n",
    "    encoder = Model(input_layer, encoder_layer2)\n",
    "    X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "    # Reconstruct data using the trained autoencoder\n",
    "    reconstructed_data = autoencoder.predict(X_test)\n",
    "\n",
    "    # Combine original test data with reconstructed data\n",
    "    X_test_combined = np.concatenate((X_test, reconstructed_data), axis=1)\n",
    "\n",
    "    # Compute cosine similarity between original and reconstructed data samples\n",
    "    cosine_similarities = cosine_similarity(X_test_combined)\n",
    "\n",
    "    # Calculate the mean cosine similarity across all samples\n",
    "    mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "    \n",
    "    return mean_cosine_similarity, encoder_layer2, input_layer\n",
    "\n",
    "# Assuming you have your data stored in X_processed and y_encoded\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the autoencoder with the decreasing learning rate schedule and RMSprop optimizer\n",
    "similarity, encoder_layer2, input_layer = train_autoencoder(X_train, X_test)\n",
    "print(\"Mean Cosine Similarity:\", similarity)\n",
    "\n",
    "# Extract features using the encoder part of the autoencoder\n",
    "encoder = Model(input_layer, encoder_layer2)\n",
    "X_encoded_train = encoder.predict(X_train)\n",
    "X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "# Print the shapes of the encoded features\n",
    "print(\"Shape of X_encoded_train:\", X_encoded_train.shape)\n",
    "print(\"Shape of X_encoded_test:\", X_encoded_test.shape)\n",
    "\n",
    "# Print some sample values of the encoded features\n",
    "print(\"Sample values of X_encoded_train:\")\n",
    "print(X_encoded_train[:5])  # Print the first 5 samples\n",
    "print(\"Sample values of X_encoded_test:\")\n",
    "print(X_encoded_test[:5])   # Print the first 5 samples\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_classifier = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier.fit(X_encoded_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_classifier.predict(X_encoded_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes y_encoded [   1    3    5 ... 1800 1801 1802]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique classes y_encoded\", np.unique(y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
