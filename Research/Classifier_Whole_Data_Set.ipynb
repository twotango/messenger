{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 16:37:25.881788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "signdata = pd.read_csv('/Users/emilkoch/Library/Mobile Documents/com~apple~CloudDocs/Data Files/signdata.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns: ['List', 'Item', 'EnglishWF(lg10)', 'SignFrequency(M)', 'SignFrequency(SD)', 'SignFrequency(Z)', 'SignFrequency(N)', 'Unknown', 'SignFrequency(M-Native)', 'SignFrequency(SD-Native)', 'SignFreq(Z-native)', 'SignFrequency(N-Native)', 'Unknown(Native)', 'SignFrequency(M-Nonnative)', 'SignFrequency(SD-Nonnative)', 'SignFrequency(N-Nonnative)', 'SignFreq(Z-Nonnative)', 'Unknown(Nonnative)', 'DominantTranslationAgreement', 'DominantTranslationAgreement(Native)', 'DominantTranslationAgreement(Nonnative)', 'Iconicity(M)', 'Iconicity(SD)', 'Iconicity(Z)', 'Iconicity(N)', 'D.Iconicity(M)', 'D.Iconicity(SD)', 'D.Iconicity(N)', 'D.Iconicity(Z)', 'D.Iconicity(M-native)', 'D.Iconicity(SD-native)', 'D.Iconicity(Z-native)', 'D.Iconicity(N-native)', 'GuessConsistency', 'GuessAccuracy', 'Transparency(M)', 'Transparency SD', 'Transparency Z', 'Initialized.2.0', 'FingerspelledLoanSign.2.0', 'Compound.2.0', 'NumberOfMorphemes.2.0', 'SignOnset(ms)', 'SignOffset(ms)', 'SignDuration(ms)', 'ClipDuration(ms)', 'MarkedHandshape.2.0', 'FlexionChange.2.0', 'Spread.2.0', 'SpreadChange.2.0', 'ThumbContact.2.0', 'RepeatedMovement.2.0', 'Contact.2.0', 'UlnarRotation.2.0', 'MarkedHandshapeM2.2.0', 'FlexionChangeM2.2.0', 'SpreadM2.2.0', 'SpreadChangeM2.2.0', 'ThumbContactM2.2.0', 'RepeatedMovementM2.2.0', 'ContactM2.2.0', 'UlnarRotationM2.2.0', 'MarkedHandshapeM3.2.0', 'FlexionChangeM3.2.0', 'SpreadM3.2.0', 'SpreadChangeM3.2.0', 'ThumbContactM3.2.0', 'RepeatedMovementM3.2.0', 'ContactM3.2.0', 'UlnarRotationM3.2.0', 'MarkedHandshapeM4.2.0', 'FlexionChangeM4.2.0', 'SpreadM4.2.0', 'SpreadChangeM4.2.0', 'ThumbContactM4.2.0', 'RepeatedMovementM4.2.0', 'ContactM4.2.0', 'NonDominantHandshapeM4.2.0', 'UlnarRotationM4.2.0', 'MarkedHandshapeM5.2.0', 'FlexionChangeM5.2.0', 'SpreadM5.2.0', 'SpreadChangeM5.2.0', 'ThumbContactM5.2.0', 'SignTypeM5.2.0', 'MovementM5.2.0', 'RepeatedMovementM5.2.0', 'MajorLocationM5.2.0', 'MinorLocationM5.2.0', 'SecondMinorLocationM5.2.0', 'ContactM5.2.0', 'NonDominantHandshapeM5.2.0', 'UlnarRotationM5.2.0', 'MarkedHandshapeM6.2.0', 'FlexionChangeM6.2.0', 'SpreadM6.2.0', 'SpreadChangeM6.2.0', 'ThumbContactM6.2.0', 'SignTypeM6.2.0', 'MovementM6.2.0', 'RepeatedMovementM6.2.0', 'MajorLocationM6.2.0', 'MinorLocationM6.2.0', 'SecondMinorLocationM6.2.0', 'ContactM6.2.0', 'NonDominantHandshapeM6.2.0', 'UlnarRotationM6.2.0', 'SignType.2.0Frequency', 'MajorLocation.2.0Frequency', 'MinorLocation.2.0Frequency', 'SecondMinorLocation.2.0Frequency', 'Movement.2.0Frequency', 'SelectedFingers.2.0Frequency', 'Flexion.2.0Frequency', 'FlexionChange.2.0Frequency', 'RepeatedMovement.2.0Frequency', 'Contact.2.0Frequency', 'Spread.2.0Frequency', 'SpreadChange.2.0Frequency', 'ThumbContact.2.0Frequency', 'ThumbPosition.2.0Frequency', 'UlnarRotation.2.0Frequency', 'Neighborhood Density 2.0', 'Parameter.Neighborhood.Density.2.0', 'PhonotacticProbability', 'Phonological Complexity', 'SignBankReferenceID', 'bglm_aoa', 'empirical_aoa']\n",
      "Categorical Columns: ['EntryID', 'LemmaID', 'Code', 'Batch', 'DominantTranslation', 'NondominantTranslations', 'Iconicity_ID', 'IconicityType', 'LexicalClass', 'Handshape.2.0', 'SelectedFingers.2.0', 'Flexion.2.0', 'ThumbPosition.2.0', 'SignType.2.0', 'Movement.2.0', 'MajorLocation.2.0', 'MinorLocation.2.0', 'SecondMinorLocation.2.0', 'NonDominantHandshape.2.0', 'HandshapeM2.2.0', 'SelectedFingersM2.2.0', 'FlexionM2.2.0', 'ThumbPositionM2.2.0', 'SignTypeM2.2.0', 'MovementM2.2.0', 'MajorLocationM2.2.0', 'MinorLocationM2.2.0', 'SecondMinorLocationM2.2.0', 'NonDominantHandshapeM2.2.0', 'HandshapeM3.2.0', 'SelectedFingersM3.2.0', 'FlexionM3.2.0', 'ThumbPositionM3.2.0', 'SignTypeM3.2.0', 'MovementM3.2.0', 'MajorLocationM3.2.0', 'MinorLocationM3.2.0', 'SecondMinorLocationM3.2.0', 'NonDominantHandshapeM3.2.0', 'HandshapeM4.2.0', 'SelectedFingersM4.2.0', 'FlexionM4.2.0', 'ThumbPositionM4.2.0', 'SignTypeM4.2.0', 'MovementM4.2.0', 'MajorLocationM4.2.0', 'MinorLocationM4.2.0', 'SecondMinorLocationM4.2.0', 'HandshapeM5.2.0', 'SelectedFingersM5.2.0', 'FlexionM5.2.0', 'ThumbPositionM5.2.0', 'HandshapeM6.2.0', 'SelectedFingersM6.2.0', 'FlexionM6.2.0', 'ThumbPositionM6.2.0', 'SignBankAnnotationID', 'SignBankLemmaID', 'SignBankSemanticField', 'InCDI', 'CDISemanticCategory']\n"
     ]
    }
   ],
   "source": [
    "# Separate target variable from features\n",
    "X = signdata.drop(columns=['SignBankEnglishTranslations'])  # Features\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n",
      "129\n",
      "   List  Item  EnglishWF(lg10)  SignFrequency(M)  SignFrequency(SD)  \\\n",
      "0     1     2            3.521             5.143              2.081   \n",
      "1     1     3            4.645             6.032              1.516   \n",
      "2     1     4            2.600             4.429              1.720   \n",
      "3     1     5            2.928             2.621              1.720   \n",
      "4     1     8            3.041             1.579              0.838   \n",
      "\n",
      "   SignFrequency(Z)  SignFrequency(N)  Unknown  SignFrequency(M-Native)  \\\n",
      "0             0.621                21    0.000                    5.167   \n",
      "1             1.068                31    0.000                    6.111   \n",
      "2             0.232                21    0.000                    4.167   \n",
      "3            -0.753                29    0.065                    2.000   \n",
      "4            -1.198                19    0.095                    1.455   \n",
      "\n",
      "   SignFrequency(SD-Native)  ...  ThumbContact.2.0Frequency  \\\n",
      "0                     2.167  ...                      0.684   \n",
      "1                     1.568  ...                      0.684   \n",
      "2                     1.899  ...                      0.684   \n",
      "3                     1.317  ...                      0.316   \n",
      "4                     0.688  ...                      0.684   \n",
      "\n",
      "   ThumbPosition.2.0Frequency  UlnarRotation.2.0Frequency  \\\n",
      "0                       0.657                       0.164   \n",
      "1                       0.657                       0.836   \n",
      "2                       0.657                       0.836   \n",
      "3                       0.343                       0.164   \n",
      "4                       0.343                       0.836   \n",
      "\n",
      "   Neighborhood Density 2.0  Parameter.Neighborhood.Density.2.0  \\\n",
      "0                         4                                 190   \n",
      "1                         5                                 391   \n",
      "2                        11                                 488   \n",
      "3                         0                                 220   \n",
      "4                         1                                 453   \n",
      "\n",
      "   PhonotacticProbability  Phonological Complexity  SignBankReferenceID  \\\n",
      "0                   0.147                      1.0                342.0   \n",
      "1                   0.099                      1.0                199.0   \n",
      "2                   0.821                      2.0               1844.0   \n",
      "3                  -0.505                      2.0               3011.0   \n",
      "4                   0.226                      2.0               2471.0   \n",
      "\n",
      "   bglm_aoa  empirical_aoa  \n",
      "0      22.0           14.0  \n",
      "1      31.0           18.0  \n",
      "2      32.0           28.0  \n",
      "3       NaN            NaN  \n",
      "4       NaN            NaN  \n",
      "\n",
      "[5 rows x 129 columns]\n",
      "List                          0\n",
      "Item                          0\n",
      "EnglishWF(lg10)             203\n",
      "SignFrequency(M)              0\n",
      "SignFrequency(SD)             0\n",
      "                           ... \n",
      "PhonotacticProbability        0\n",
      "Phonological Complexity       3\n",
      "SignBankReferenceID           0\n",
      "bglm_aoa                   1515\n",
      "empirical_aoa              1515\n",
      "Length: 129, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for numerical features\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Copy numerical columns\n",
    "X_numerical = X[numerical_cols].copy()\n",
    "print(len(X_numerical))\n",
    "print(len(numerical_cols))\n",
    "print(X_numerical.head())\n",
    "print(X_numerical.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['RepeatedMovementM4.2.0' 'UlnarRotationM4.2.0' 'FlexionChangeM5.2.0'\n",
      " 'SpreadChangeM5.2.0' 'SignTypeM5.2.0' 'MovementM5.2.0'\n",
      " 'RepeatedMovementM5.2.0' 'MajorLocationM5.2.0' 'MinorLocationM5.2.0'\n",
      " 'SecondMinorLocationM5.2.0' 'ContactM5.2.0' 'NonDominantHandshapeM5.2.0'\n",
      " 'UlnarRotationM5.2.0' 'MarkedHandshapeM6.2.0' 'FlexionChangeM6.2.0'\n",
      " 'SpreadM6.2.0' 'SpreadChangeM6.2.0' 'ThumbContactM6.2.0' 'SignTypeM6.2.0'\n",
      " 'MovementM6.2.0' 'RepeatedMovementM6.2.0' 'MajorLocationM6.2.0'\n",
      " 'MinorLocationM6.2.0' 'SecondMinorLocationM6.2.0' 'ContactM6.2.0'\n",
      " 'NonDominantHandshapeM6.2.0' 'UlnarRotationM6.2.0']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values and scaling\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_numerical_imputed = imputer.fit_transform(X_numerical) \n",
    "scaler = StandardScaler()\n",
    "X_numerical_scaled  = scaler.fit_transform(X_numerical_imputed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['HandshapeM6.2.0' 'SelectedFingersM6.2.0' 'FlexionM6.2.0']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "categorical_imputer = SimpleImputer(strategy='most_frequent', add_indicator=False)\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# Copy categorical columns\n",
    "X_categorical = X[categorical_cols].copy()\n",
    "\n",
    "X_categorical_imputed = categorical_imputer.fit_transform(X_categorical)\n",
    "\n",
    "# Encode categorical features\n",
    "encoded_cols = pd.DataFrame(encoder.fit_transform(X_categorical))\n",
    "encoded_cols.columns = encoder.get_feature_names_out(categorical_cols)\n",
    "categorical_cols_encoded = encoded_cols.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate numerical and encoded categorical columns\n",
    "X_processed = pd.concat([pd.DataFrame(X_numerical_scaled), encoded_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'SignBankEnglishTranslations' column after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize SimpleImputer\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fill missing values in the target variable\n",
    "y_imputed = imputer.fit_transform(signdata[['SignBankEnglishTranslations']])\n",
    "\n",
    "# Convert the NumPy array back to a pandas Series\n",
    "y_imputed = pd.Series(y_imputed.flatten())\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target variable\n",
    "y_encoded = label_encoder.fit_transform(y_imputed)\n",
    "\n",
    "# Check for NaN values in the target variable after imputation\n",
    "nan_count_after_impute = pd.Series(y_imputed).isnull().sum()\n",
    "print(\"Number of NaN values in 'SignBankEnglishTranslations' column after imputation:\", nan_count_after_impute)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_train_encoded: [   0    1    2 ... 1467 1468 1469]\n",
      "Shape of y_train_encoded: (1587,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to y_train\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Check the unique values and shape of y_train_encoded\n",
    "print(\"Unique values in y_train_encoded:\", np.unique(y_train_encoded))\n",
    "print(\"Shape of y_train_encoded:\", y_train_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the decreasing learning rate schedule\n",
    "def decreasing_schedule(epoch):\n",
    "    return 0.001 * np.exp(-0.1 * epoch)\n",
    "\n",
    "def train_autoencoder(X_train, X_test):\n",
    "    # Define the autoencoder architecture\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = 64  # Adjust as needed\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder_layer1 = Dense(128, activation='relu')(input_layer)\n",
    "    encoder_layer1 = BatchNormalization()(encoder_layer1)\n",
    "    encoder_layer1 = Dropout(0.5)(encoder_layer1)\n",
    "\n",
    "    encoder_layer2 = Dense(encoding_dim, activation='relu')(encoder_layer1)\n",
    "    encoder_layer2 = BatchNormalization()(encoder_layer2)\n",
    "    encoder_layer2 = Dropout(0.5)(encoder_layer2)\n",
    "\n",
    "    decoder_layer1 = Dense(128, activation='relu')(encoder_layer2)\n",
    "    decoder_layer1 = BatchNormalization()(decoder_layer1)\n",
    "\n",
    "    decoder_layer2 = Dense(input_dim, activation='sigmoid')(decoder_layer1)  # Adjusted output dimensionality\n",
    "    decoder_layer2 = Dropout(0.5)(decoder_layer2)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoder_layer2)\n",
    "\n",
    "    # Define the optimizer with RMSprop\n",
    "    optimizer = RMSprop(learning_rate=0.001) \n",
    "\n",
    "    # Compile the autoencoder model with RMSprop optimizer\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Define the learning rate scheduler callback\n",
    "    lr_scheduler = LearningRateScheduler(decreasing_schedule)\n",
    "\n",
    "    # Train the autoencoder with learning rate scheduler\n",
    "    autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, \n",
    "                    validation_data=(X_test, X_test), callbacks=[lr_scheduler])\n",
    "\n",
    "    # Extract features using the encoder part of the autoencoder\n",
    "    encoder = Model(input_layer, encoder_layer2)\n",
    "    X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "    # Reconstruct data using the trained autoencoder\n",
    "    reconstructed_data = autoencoder.predict(X_test)\n",
    "\n",
    "    # Combine original test data with reconstructed data\n",
    "    X_test_combined = np.concatenate((X_test, reconstructed_data), axis=1)\n",
    "\n",
    "    # Compute cosine similarity between original and reconstructed data samples\n",
    "    cosine_similarities = cosine_similarity(X_test_combined)\n",
    "\n",
    "    # Calculate the mean cosine similarity across all samples\n",
    "    mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "    \n",
    "    return mean_cosine_similarity, encoder_layer2, input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_subset: (1984,)\n"
     ]
    }
   ],
   "source": [
    "y_subset = y_encoded[:len(X_processed)]\n",
    "\n",
    "# Check the shape of the subsetted y\n",
    "print(\"Shape of y_subset:\", y_subset.shape)\n",
    "\n",
    "# Now, both X_processed and y_subset should have the same number of samples\n",
    "# You can proceed with splitting them into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_subset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 8s 75ms/step - loss: 0.5091 - val_loss: 0.2569 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.5053 - val_loss: 0.2552 - lr: 9.0484e-04\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 2s 36ms/step - loss: 0.5016 - val_loss: 0.2534 - lr: 8.1873e-04\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 2s 40ms/step - loss: 0.4982 - val_loss: 0.2511 - lr: 7.4082e-04\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 2s 38ms/step - loss: 0.4933 - val_loss: 0.2483 - lr: 6.7032e-04\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.4872 - val_loss: 0.2448 - lr: 6.0653e-04\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 2s 43ms/step - loss: 0.4803 - val_loss: 0.2411 - lr: 5.4881e-04\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 2s 43ms/step - loss: 0.4719 - val_loss: 0.2366 - lr: 4.9659e-04\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 2s 40ms/step - loss: 0.4625 - val_loss: 0.2321 - lr: 4.4933e-04\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 2s 40ms/step - loss: 0.4526 - val_loss: 0.2269 - lr: 4.0657e-04\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.4424 - val_loss: 0.2213 - lr: 3.6788e-04\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 3s 64ms/step - loss: 0.4316 - val_loss: 0.2160 - lr: 3.3287e-04\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.4209 - val_loss: 0.2110 - lr: 3.0119e-04\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.4108 - val_loss: 0.2057 - lr: 2.7253e-04\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 3s 68ms/step - loss: 0.4005 - val_loss: 0.2008 - lr: 2.4660e-04\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.3910 - val_loss: 0.1961 - lr: 2.2313e-04\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 2s 43ms/step - loss: 0.3820 - val_loss: 0.1918 - lr: 2.0190e-04\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.3734 - val_loss: 0.1873 - lr: 1.8268e-04\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.3655 - val_loss: 0.1832 - lr: 1.6530e-04\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.3580 - val_loss: 0.1795 - lr: 1.4957e-04\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.3512 - val_loss: 0.1763 - lr: 1.3534e-04\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.3451 - val_loss: 0.1732 - lr: 1.2246e-04\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 3s 51ms/step - loss: 0.3391 - val_loss: 0.1706 - lr: 1.1080e-04\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.3340 - val_loss: 0.1682 - lr: 1.0026e-04\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 2s 35ms/step - loss: 0.3293 - val_loss: 0.1661 - lr: 9.0718e-05\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 2s 35ms/step - loss: 0.3250 - val_loss: 0.1640 - lr: 8.2085e-05\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 2s 40ms/step - loss: 0.3209 - val_loss: 0.1620 - lr: 7.4274e-05\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 2s 35ms/step - loss: 0.3172 - val_loss: 0.1601 - lr: 6.7206e-05\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 2s 35ms/step - loss: 0.3140 - val_loss: 0.1585 - lr: 6.0810e-05\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 2s 38ms/step - loss: 0.3110 - val_loss: 0.1572 - lr: 5.5023e-05\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 2s 37ms/step - loss: 0.3084 - val_loss: 0.1559 - lr: 4.9787e-05\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 2s 42ms/step - loss: 0.3059 - val_loss: 0.1546 - lr: 4.5049e-05\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 2s 35ms/step - loss: 0.3036 - val_loss: 0.1535 - lr: 4.0762e-05\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 2s 41ms/step - loss: 0.3017 - val_loss: 0.1527 - lr: 3.6883e-05\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 2s 41ms/step - loss: 0.2998 - val_loss: 0.1516 - lr: 3.3373e-05\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 2s 35ms/step - loss: 0.2980 - val_loss: 0.1510 - lr: 3.0197e-05\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 2s 41ms/step - loss: 0.2966 - val_loss: 0.1504 - lr: 2.7324e-05\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 2s 43ms/step - loss: 0.2954 - val_loss: 0.1498 - lr: 2.4724e-05\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.2940 - val_loss: 0.1492 - lr: 2.2371e-05\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.2929 - val_loss: 0.1485 - lr: 2.0242e-05\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 3s 54ms/step - loss: 0.2918 - val_loss: 0.1480 - lr: 1.8316e-05\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.2909 - val_loss: 0.1476 - lr: 1.6573e-05\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.2902 - val_loss: 0.1472 - lr: 1.4996e-05\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 2s 43ms/step - loss: 0.2895 - val_loss: 0.1468 - lr: 1.3569e-05\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 2s 36ms/step - loss: 0.2888 - val_loss: 0.1465 - lr: 1.2277e-05\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 2s 36ms/step - loss: 0.2881 - val_loss: 0.1463 - lr: 1.1109e-05\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 2s 39ms/step - loss: 0.2875 - val_loss: 0.1460 - lr: 1.0052e-05\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 2s 43ms/step - loss: 0.2871 - val_loss: 0.1456 - lr: 9.0953e-06\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 3s 68ms/step - loss: 0.2867 - val_loss: 0.1454 - lr: 8.2297e-06\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 0.2864 - val_loss: 0.1453 - lr: 7.4466e-06\n",
      "13/13 [==============================] - 0s 6ms/step\n",
      "13/13 [==============================] - 0s 8ms/step\n",
      "Mean Cosine Similarity: 0.9349847123544542\n",
      "50/50 [==============================] - 1s 8ms/step\n",
      "13/13 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Train the autoencoder with the decreasing learning rate schedule and RMSprop optimizer\n",
    "similarity, encoder_layer2, input_layer = train_autoencoder(X_train, X_test)\n",
    "print(\"Mean Cosine Similarity:\", similarity)\n",
    "\n",
    "# Extract features using the encoder part of the autoencoder\n",
    "encoder = Model(input_layer, encoder_layer2)\n",
    "X_encoded_train = encoder.predict(X_train)\n",
    "X_encoded_test = encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 5s 53ms/step - loss: 0.5091 - val_loss: 0.2570 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 2s 42ms/step - loss: 0.5054 - val_loss: 0.2555 - lr: 9.0484e-04\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 2s 44ms/step - loss: 0.5023 - val_loss: 0.2540 - lr: 8.1873e-04\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.4985 - val_loss: 0.2523 - lr: 7.4082e-04\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 2s 42ms/step - loss: 0.4942 - val_loss: 0.2502 - lr: 6.7032e-04\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 2s 41ms/step - loss: 0.4892 - val_loss: 0.2474 - lr: 6.0653e-04\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 2s 42ms/step - loss: 0.4826 - val_loss: 0.2439 - lr: 5.4881e-04\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 2s 44ms/step - loss: 0.4754 - val_loss: 0.2398 - lr: 4.9659e-04\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.4668 - val_loss: 0.2352 - lr: 4.4933e-04\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 2s 44ms/step - loss: 0.4581 - val_loss: 0.2304 - lr: 4.0657e-04\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 2s 43ms/step - loss: 0.4486 - val_loss: 0.2252 - lr: 3.6788e-04\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.4390 - val_loss: 0.2204 - lr: 3.3287e-04\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.4293 - val_loss: 0.2154 - lr: 3.0119e-04\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 2s 44ms/step - loss: 0.4197 - val_loss: 0.2109 - lr: 2.7253e-04\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.4102 - val_loss: 0.2065 - lr: 2.4660e-04\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.4017 - val_loss: 0.2020 - lr: 2.2313e-04\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.3930 - val_loss: 0.1979 - lr: 2.0190e-04\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 2s 44ms/step - loss: 0.3850 - val_loss: 0.1943 - lr: 1.8268e-04\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.3775 - val_loss: 0.1906 - lr: 1.6530e-04\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.3706 - val_loss: 0.1870 - lr: 1.4957e-04\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.3641 - val_loss: 0.1840 - lr: 1.3534e-04\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 2s 44ms/step - loss: 0.3579 - val_loss: 0.1809 - lr: 1.2246e-04\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.3526 - val_loss: 0.1783 - lr: 1.1080e-04\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 2s 44ms/step - loss: 0.3475 - val_loss: 0.1759 - lr: 1.0026e-04\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.3432 - val_loss: 0.1738 - lr: 9.0718e-05\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.3389 - val_loss: 0.1718 - lr: 8.2085e-05\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 2s 44ms/step - loss: 0.3349 - val_loss: 0.1699 - lr: 7.4274e-05\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 2s 44ms/step - loss: 0.3315 - val_loss: 0.1681 - lr: 6.7206e-05\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.3284 - val_loss: 0.1665 - lr: 6.0810e-05\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 3s 52ms/step - loss: 0.3254 - val_loss: 0.1653 - lr: 5.5023e-05\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.3226 - val_loss: 0.1641 - lr: 4.9787e-05\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 3s 52ms/step - loss: 0.3203 - val_loss: 0.1630 - lr: 4.5049e-05\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.3183 - val_loss: 0.1619 - lr: 4.0762e-05\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.3163 - val_loss: 0.1609 - lr: 3.6883e-05\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 2s 44ms/step - loss: 0.3144 - val_loss: 0.1602 - lr: 3.3373e-05\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.3128 - val_loss: 0.1593 - lr: 3.0197e-05\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 3s 50ms/step - loss: 0.3113 - val_loss: 0.1586 - lr: 2.7324e-05\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 2s 44ms/step - loss: 0.3100 - val_loss: 0.1579 - lr: 2.4724e-05\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.3088 - val_loss: 0.1574 - lr: 2.2371e-05\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.3077 - val_loss: 0.1567 - lr: 2.0242e-05\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.3067 - val_loss: 0.1562 - lr: 1.8316e-05\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 2s 50ms/step - loss: 0.3059 - val_loss: 0.1559 - lr: 1.6573e-05\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.3050 - val_loss: 0.1555 - lr: 1.4996e-05\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.3043 - val_loss: 0.1552 - lr: 1.3569e-05\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.3036 - val_loss: 0.1550 - lr: 1.2277e-05\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 3s 54ms/step - loss: 0.3030 - val_loss: 0.1546 - lr: 1.1109e-05\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.3024 - val_loss: 0.1543 - lr: 1.0052e-05\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 2s 44ms/step - loss: 0.3019 - val_loss: 0.1541 - lr: 9.0953e-06\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.3015 - val_loss: 0.1540 - lr: 8.2297e-06\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 3s 52ms/step - loss: 0.3012 - val_loss: 0.1538 - lr: 7.4466e-06\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Train the autoencoder and extract encoded features\n",
    "mean_cosine_similarity, encoder_layer2, input_layer = train_autoencoder(X_train, X_test)\n",
    "\n",
    "# Extract encoded features using the encoder part of the autoencoder\n",
    "encoder = Model(input_layer, encoder_layer2)\n",
    "X_encoded_train = encoder.predict(X_train)\n",
    "X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "# Train the model using the encoded features and target variable\n",
    "model.fit(X_encoded_train, y_train_encoded)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_encoded_test)\n",
    "\n",
    "# Evaluate the model performance (e.g., accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_train: [   0    1    2 ... 1799 1800 1802]\n",
      "Expected classes based on unique values: [   0    1    2 ... 1467 1468 1469]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check Unique Values in y_train\n",
    "unique_values_y_train = np.unique(y_train)\n",
    "print(\"Unique values in y_train:\", unique_values_y_train)\n",
    "\n",
    "# Step 2: Compare with Expected Classes\n",
    "expected_classes = np.arange(len(unique_values_y_train))\n",
    "print(\"Expected classes based on unique values:\", expected_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
