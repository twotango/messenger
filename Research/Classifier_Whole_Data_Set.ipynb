{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "signdata = pd.read_csv('/Users/emilkoch/Library/Mobile Documents/com~apple~CloudDocs/Data Files/signdata.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns: ['List', 'Item', 'EnglishWF(lg10)', 'SignFrequency(M)', 'SignFrequency(SD)', 'SignFrequency(Z)', 'SignFrequency(N)', 'Unknown', 'SignFrequency(M-Native)', 'SignFrequency(SD-Native)', 'SignFreq(Z-native)', 'SignFrequency(N-Native)', 'Unknown(Native)', 'SignFrequency(M-Nonnative)', 'SignFrequency(SD-Nonnative)', 'SignFrequency(N-Nonnative)', 'SignFreq(Z-Nonnative)', 'Unknown(Nonnative)', 'DominantTranslationAgreement', 'DominantTranslationAgreement(Native)', 'DominantTranslationAgreement(Nonnative)', 'Iconicity(M)', 'Iconicity(SD)', 'Iconicity(Z)', 'Iconicity(N)', 'D.Iconicity(M)', 'D.Iconicity(SD)', 'D.Iconicity(N)', 'D.Iconicity(Z)', 'D.Iconicity(M-native)', 'D.Iconicity(SD-native)', 'D.Iconicity(Z-native)', 'D.Iconicity(N-native)', 'GuessConsistency', 'GuessAccuracy', 'Transparency(M)', 'Transparency SD', 'Transparency Z', 'Initialized.2.0', 'FingerspelledLoanSign.2.0', 'Compound.2.0', 'NumberOfMorphemes.2.0', 'SignOnset(ms)', 'SignOffset(ms)', 'SignDuration(ms)', 'ClipDuration(ms)', 'MarkedHandshape.2.0', 'FlexionChange.2.0', 'Spread.2.0', 'SpreadChange.2.0', 'ThumbContact.2.0', 'RepeatedMovement.2.0', 'Contact.2.0', 'UlnarRotation.2.0', 'MarkedHandshapeM2.2.0', 'FlexionChangeM2.2.0', 'SpreadM2.2.0', 'SpreadChangeM2.2.0', 'ThumbContactM2.2.0', 'RepeatedMovementM2.2.0', 'ContactM2.2.0', 'UlnarRotationM2.2.0', 'MarkedHandshapeM3.2.0', 'FlexionChangeM3.2.0', 'SpreadM3.2.0', 'SpreadChangeM3.2.0', 'ThumbContactM3.2.0', 'RepeatedMovementM3.2.0', 'ContactM3.2.0', 'UlnarRotationM3.2.0', 'MarkedHandshapeM4.2.0', 'FlexionChangeM4.2.0', 'SpreadM4.2.0', 'SpreadChangeM4.2.0', 'ThumbContactM4.2.0', 'RepeatedMovementM4.2.0', 'ContactM4.2.0', 'NonDominantHandshapeM4.2.0', 'UlnarRotationM4.2.0', 'MarkedHandshapeM5.2.0', 'FlexionChangeM5.2.0', 'SpreadM5.2.0', 'SpreadChangeM5.2.0', 'ThumbContactM5.2.0', 'SignTypeM5.2.0', 'MovementM5.2.0', 'RepeatedMovementM5.2.0', 'MajorLocationM5.2.0', 'MinorLocationM5.2.0', 'SecondMinorLocationM5.2.0', 'ContactM5.2.0', 'NonDominantHandshapeM5.2.0', 'UlnarRotationM5.2.0', 'MarkedHandshapeM6.2.0', 'FlexionChangeM6.2.0', 'SpreadM6.2.0', 'SpreadChangeM6.2.0', 'ThumbContactM6.2.0', 'SignTypeM6.2.0', 'MovementM6.2.0', 'RepeatedMovementM6.2.0', 'MajorLocationM6.2.0', 'MinorLocationM6.2.0', 'SecondMinorLocationM6.2.0', 'ContactM6.2.0', 'NonDominantHandshapeM6.2.0', 'UlnarRotationM6.2.0', 'SignType.2.0Frequency', 'MajorLocation.2.0Frequency', 'MinorLocation.2.0Frequency', 'SecondMinorLocation.2.0Frequency', 'Movement.2.0Frequency', 'SelectedFingers.2.0Frequency', 'Flexion.2.0Frequency', 'FlexionChange.2.0Frequency', 'RepeatedMovement.2.0Frequency', 'Contact.2.0Frequency', 'Spread.2.0Frequency', 'SpreadChange.2.0Frequency', 'ThumbContact.2.0Frequency', 'ThumbPosition.2.0Frequency', 'UlnarRotation.2.0Frequency', 'Neighborhood Density 2.0', 'Parameter.Neighborhood.Density.2.0', 'PhonotacticProbability', 'Phonological Complexity', 'SignBankReferenceID', 'bglm_aoa', 'empirical_aoa']\n",
      "Categorical Columns: ['EntryID', 'LemmaID', 'Code', 'Batch', 'DominantTranslation', 'NondominantTranslations', 'Iconicity_ID', 'IconicityType', 'LexicalClass', 'Handshape.2.0', 'SelectedFingers.2.0', 'Flexion.2.0', 'ThumbPosition.2.0', 'SignType.2.0', 'Movement.2.0', 'MajorLocation.2.0', 'MinorLocation.2.0', 'SecondMinorLocation.2.0', 'NonDominantHandshape.2.0', 'HandshapeM2.2.0', 'SelectedFingersM2.2.0', 'FlexionM2.2.0', 'ThumbPositionM2.2.0', 'SignTypeM2.2.0', 'MovementM2.2.0', 'MajorLocationM2.2.0', 'MinorLocationM2.2.0', 'SecondMinorLocationM2.2.0', 'NonDominantHandshapeM2.2.0', 'HandshapeM3.2.0', 'SelectedFingersM3.2.0', 'FlexionM3.2.0', 'ThumbPositionM3.2.0', 'SignTypeM3.2.0', 'MovementM3.2.0', 'MajorLocationM3.2.0', 'MinorLocationM3.2.0', 'SecondMinorLocationM3.2.0', 'NonDominantHandshapeM3.2.0', 'HandshapeM4.2.0', 'SelectedFingersM4.2.0', 'FlexionM4.2.0', 'ThumbPositionM4.2.0', 'SignTypeM4.2.0', 'MovementM4.2.0', 'MajorLocationM4.2.0', 'MinorLocationM4.2.0', 'SecondMinorLocationM4.2.0', 'HandshapeM5.2.0', 'SelectedFingersM5.2.0', 'FlexionM5.2.0', 'ThumbPositionM5.2.0', 'HandshapeM6.2.0', 'SelectedFingersM6.2.0', 'FlexionM6.2.0', 'ThumbPositionM6.2.0', 'SignBankAnnotationID', 'SignBankLemmaID', 'SignBankSemanticField', 'InCDI', 'CDISemanticCategory']\n"
     ]
    }
   ],
   "source": [
    "# Separate target variable from features\n",
    "X = signdata.drop(columns=['SignBankEnglishTranslations'])  # Features\n",
    "y = signdata['SignBankEnglishTranslations']\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2723\n",
      "129\n",
      "   List  Item  EnglishWF(lg10)  SignFrequency(M)  SignFrequency(SD)  \\\n",
      "0     1     2            3.521             5.143              2.081   \n",
      "1     1     3            4.645             6.032              1.516   \n",
      "2     1     4            2.600             4.429              1.720   \n",
      "3     1     5            2.928             2.621              1.720   \n",
      "4     1     8            3.041             1.579              0.838   \n",
      "\n",
      "   SignFrequency(Z)  SignFrequency(N)  Unknown  SignFrequency(M-Native)  \\\n",
      "0             0.621                21    0.000                    5.167   \n",
      "1             1.068                31    0.000                    6.111   \n",
      "2             0.232                21    0.000                    4.167   \n",
      "3            -0.753                29    0.065                    2.000   \n",
      "4            -1.198                19    0.095                    1.455   \n",
      "\n",
      "   SignFrequency(SD-Native)  ...  ThumbContact.2.0Frequency  \\\n",
      "0                     2.167  ...                      0.684   \n",
      "1                     1.568  ...                      0.684   \n",
      "2                     1.899  ...                      0.684   \n",
      "3                     1.317  ...                      0.316   \n",
      "4                     0.688  ...                      0.684   \n",
      "\n",
      "   ThumbPosition.2.0Frequency  UlnarRotation.2.0Frequency  \\\n",
      "0                       0.657                       0.164   \n",
      "1                       0.657                       0.836   \n",
      "2                       0.657                       0.836   \n",
      "3                       0.343                       0.164   \n",
      "4                       0.343                       0.836   \n",
      "\n",
      "   Neighborhood Density 2.0  Parameter.Neighborhood.Density.2.0  \\\n",
      "0                         4                                 190   \n",
      "1                         5                                 391   \n",
      "2                        11                                 488   \n",
      "3                         0                                 220   \n",
      "4                         1                                 453   \n",
      "\n",
      "   PhonotacticProbability  Phonological Complexity  SignBankReferenceID  \\\n",
      "0                   0.147                      1.0                342.0   \n",
      "1                   0.099                      1.0                199.0   \n",
      "2                   0.821                      2.0               1844.0   \n",
      "3                  -0.505                      2.0               3011.0   \n",
      "4                   0.226                      2.0               2471.0   \n",
      "\n",
      "   bglm_aoa  empirical_aoa  \n",
      "0      22.0           14.0  \n",
      "1      31.0           18.0  \n",
      "2      32.0           28.0  \n",
      "3       NaN            NaN  \n",
      "4       NaN            NaN  \n",
      "\n",
      "[5 rows x 129 columns]\n",
      "List                          0\n",
      "Item                          0\n",
      "EnglishWF(lg10)             334\n",
      "SignFrequency(M)              0\n",
      "SignFrequency(SD)             0\n",
      "                           ... \n",
      "PhonotacticProbability        0\n",
      "Phonological Complexity      26\n",
      "SignBankReferenceID         734\n",
      "bglm_aoa                   2190\n",
      "empirical_aoa              2190\n",
      "Length: 129, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for numerical features\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Copy numerical columns\n",
    "X_numerical = X[numerical_cols].copy()\n",
    "print(len(X_numerical))\n",
    "print(len(numerical_cols))\n",
    "print(X_numerical.head())\n",
    "print(X_numerical.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['UlnarRotationM4.2.0' 'FlexionChangeM5.2.0' 'SpreadChangeM5.2.0'\n",
      " 'SignTypeM5.2.0' 'MovementM5.2.0' 'RepeatedMovementM5.2.0'\n",
      " 'MajorLocationM5.2.0' 'MinorLocationM5.2.0' 'SecondMinorLocationM5.2.0'\n",
      " 'ContactM5.2.0' 'NonDominantHandshapeM5.2.0' 'UlnarRotationM5.2.0'\n",
      " 'FlexionChangeM6.2.0' 'SpreadChangeM6.2.0' 'SignTypeM6.2.0'\n",
      " 'MovementM6.2.0' 'RepeatedMovementM6.2.0' 'MajorLocationM6.2.0'\n",
      " 'MinorLocationM6.2.0' 'SecondMinorLocationM6.2.0' 'ContactM6.2.0'\n",
      " 'NonDominantHandshapeM6.2.0' 'UlnarRotationM6.2.0']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values and scaling\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_numerical_imputed = imputer.fit_transform(X_numerical) \n",
    "scaler = StandardScaler()\n",
    "X_numerical_scaled  = scaler.fit_transform(X_numerical_imputed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for categorical features\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# Copy categorical columns\n",
    "X_categorical = X[categorical_cols].copy()\n",
    "\n",
    "# Impute missing values in categorical columns\n",
    "X_categorical = pd.DataFrame(categorical_imputer.fit_transform(X_categorical), columns=X_categorical.columns)\n",
    "\n",
    "# Encode categorical features\n",
    "encoded_cols = pd.DataFrame(encoder.fit_transform(X_categorical))\n",
    "encoded_cols.columns = encoder.get_feature_names_out(categorical_cols)\n",
    "categorical_cols_encoded = encoded_cols.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate numerical and encoded categorical columns\n",
    "X_processed = pd.concat([pd.DataFrame(X_numerical_scaled), encoded_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'SignBankEnglishTranslations' column: 739\n",
      "Number of NaN values in 'SignBankEnglishTranslations' column after imputation: 0\n",
      "object\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in the target variable\n",
    "nan_count = signdata['SignBankEnglishTranslations'].isnull().sum()\n",
    "print(\"Number of NaN values in 'SignBankEnglishTranslations' column:\", nan_count)\n",
    "\n",
    "# Initialize SimpleImputer\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Apply SimpleImputer to fill missing values in the target variable\n",
    "filled_values = imputer.fit_transform(signdata[['SignBankEnglishTranslations']])\n",
    "y_imputed = filled_values.flatten()  # Flatten the 2D array to 1D before assigning back to the Series\n",
    "\n",
    "# Check for NaN values in the target variable after imputation\n",
    "nan_count_after_impute = pd.Series(y_imputed).isnull().sum()\n",
    "print(\"Number of NaN values in 'SignBankEnglishTranslations' column after imputation:\", nan_count_after_impute)\n",
    "print(y_imputed.dtype)\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to the target variable 'SignBankEnglishTranslations'\n",
    "y_encoded = label_encoder.fit_transform(y_imputed)\n",
    "print(y_encoded.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using features with significant correlations: 0.28440366972477066\n",
      "Accuracy using randomly mixed features: 0.3467889908256881\n"
     ]
    }
   ],
   "source": [
    "# Define X_train_corr and X_test_corr using the features from significant_correlations_df\n",
    "X_train_corr = X_train[significant_correlations_df['Feature']]\n",
    "X_test_corr = X_test[significant_correlations_df['Feature']]\n",
    "\n",
    "# Define X_train_rf and X_test_rf using the selected features\n",
    "X_train_rf = X_train[selected_features]\n",
    "X_test_rf = X_test[selected_features]\n",
    "\n",
    "# Train Random Forest using significant correlations\n",
    "rf_corr_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_corr_classifier.fit(X_train_corr, y_train)\n",
    "\n",
    "# Train Random Forest using randomly mixed features\n",
    "rf_rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_rf_classifier.fit(X_train_rf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_corr = rf_corr_classifier.predict(X_test_corr)\n",
    "y_pred_rf = rf_rf_classifier.predict(X_test_rf)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_corr = accuracy_score(y_test, y_pred_corr)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Accuracy using features with significant correlations:\", accuracy_corr)\n",
    "print(\"Accuracy using randomly mixed features:\", accuracy_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "69/69 [==============================] - 3s 35ms/step - loss: 0.0764 - val_loss: 0.0079\n",
      "Epoch 2/50\n",
      "69/69 [==============================] - 2s 23ms/step - loss: 0.0082 - val_loss: 0.0079\n",
      "Epoch 3/50\n",
      "69/69 [==============================] - 2s 28ms/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 4/50\n",
      "69/69 [==============================] - 3s 39ms/step - loss: 0.0081 - val_loss: 0.0078\n",
      "Epoch 5/50\n",
      "69/69 [==============================] - 2s 22ms/step - loss: 0.0079 - val_loss: 0.0076\n",
      "Epoch 6/50\n",
      "69/69 [==============================] - 2s 23ms/step - loss: 0.0078 - val_loss: 0.0074\n",
      "Epoch 7/50\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0076 - val_loss: 0.0073\n",
      "Epoch 8/50\n",
      "69/69 [==============================] - 2s 24ms/step - loss: 0.0075 - val_loss: 0.0072\n",
      "Epoch 9/50\n",
      "69/69 [==============================] - 2s 29ms/step - loss: 0.0073 - val_loss: 0.0070\n",
      "Epoch 10/50\n",
      "69/69 [==============================] - 2s 24ms/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 11/50\n",
      "69/69 [==============================] - 2s 28ms/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 12/50\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 13/50\n",
      "69/69 [==============================] - 2s 34ms/step - loss: 0.0068 - val_loss: 0.0066\n",
      "Epoch 14/50\n",
      "69/69 [==============================] - 2s 34ms/step - loss: 0.0067 - val_loss: 0.0065\n",
      "Epoch 15/50\n",
      "69/69 [==============================] - 2s 36ms/step - loss: 0.0067 - val_loss: 0.0064\n",
      "Epoch 16/50\n",
      "69/69 [==============================] - 3s 41ms/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 17/50\n",
      "69/69 [==============================] - 3s 40ms/step - loss: 0.0065 - val_loss: 0.0062\n",
      "Epoch 18/50\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.0064 - val_loss: 0.0062\n",
      "Epoch 19/50\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.0063 - val_loss: 0.0061\n",
      "Epoch 20/50\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 0.0062 - val_loss: 0.0060\n",
      "Epoch 21/50\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 0.0062 - val_loss: 0.0059\n",
      "Epoch 22/50\n",
      "69/69 [==============================] - 2s 29ms/step - loss: 0.0061 - val_loss: 0.0059\n",
      "Epoch 23/50\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 24/50\n",
      "69/69 [==============================] - 2s 30ms/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 25/50\n",
      "69/69 [==============================] - 2s 29ms/step - loss: 0.0059 - val_loss: 0.0057\n",
      "Epoch 26/50\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0059 - val_loss: 0.0057\n",
      "Epoch 27/50\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 28/50\n",
      "69/69 [==============================] - 2s 24ms/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 29/50\n",
      "69/69 [==============================] - 2s 27ms/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 30/50\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 31/50\n",
      "69/69 [==============================] - 2s 29ms/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 32/50\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 33/50\n",
      "69/69 [==============================] - 2s 24ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 34/50\n",
      "69/69 [==============================] - 2s 33ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 35/50\n",
      "69/69 [==============================] - 2s 30ms/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 36/50\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 37/50\n",
      "69/69 [==============================] - 2s 34ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 38/50\n",
      "69/69 [==============================] - 2s 36ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 39/50\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 40/50\n",
      "69/69 [==============================] - 2s 22ms/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 41/50\n",
      "69/69 [==============================] - 2s 32ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 42/50\n",
      "69/69 [==============================] - 2s 26ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 43/50\n",
      "69/69 [==============================] - 2s 30ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 44/50\n",
      "69/69 [==============================] - 2s 24ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 45/50\n",
      "69/69 [==============================] - 2s 27ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 46/50\n",
      "69/69 [==============================] - 2s 23ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 47/50\n",
      "69/69 [==============================] - 2s 23ms/step - loss: 0.0054 - val_loss: 0.0052\n",
      "Epoch 48/50\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 49/50\n",
      "69/69 [==============================] - 2s 25ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 50/50\n",
      "69/69 [==============================] - 2s 24ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "69/69 [==============================] - 1s 4ms/step\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "18/18 [==============================] - 0s 9ms/step\n",
      "Mean Cosine Similarity: 0.46733752473965223\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have your data stored in X_processed and y_encoded\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 64  # Adjust as needed\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoder_layer = Dense(input_dim, activation='sigmoid')(encoder_layer)\n",
    "\n",
    "autoencoder = Model(input_layer, decoder_layer)\n",
    "\n",
    "# Compile the autoencoder model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, validation_data=(X_test, X_test))\n",
    "\n",
    "# Extract features using the encoder part of the autoencoder\n",
    "encoder = Model(input_layer, encoder_layer)\n",
    "X_encoded_train = encoder.predict(X_train)\n",
    "X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "# Reconstruct data using the trained autoencoder\n",
    "reconstructed_data = autoencoder.predict(X_test)\n",
    "\n",
    "# Combine original test data with reconstructed data\n",
    "X_test_combined = np.concatenate((X_test, reconstructed_data), axis=1)\n",
    "\n",
    "# Compute cosine similarity between original and reconstructed data samples\n",
    "cosine_similarities = cosine_similarity(X_test_combined)\n",
    "\n",
    "# Calculate the mean cosine similarity across all samples\n",
    "mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "print(\"Mean Cosine Similarity:\", mean_cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'optimizer': 'rmsprop'}\n",
      "Best Score:  0.4870285264335325\n"
     ]
    }
   ],
   "source": [
    "# Define a function to create the autoencoder model\n",
    "def create_autoencoder(optimizer='adam'):\n",
    "    # Initialize optimizer based on the provided string\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = 'adam'\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop()\n",
    "    elif optimizer == 'adadelta':\n",
    "        optimizer = Adadelta()\n",
    "    elif optimizer == 'adagrad':\n",
    "        optimizer = Adagrad()\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoder_layer = Dense(input_dim, activation='sigmoid')(encoder_layer)\n",
    "    autoencoder = Model(input_layer, decoder_layer)\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return autoencoder\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = KerasRegressor(build_fn=create_autoencoder, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Define the hyperparameter grid including additional optimizers\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'rmsprop', 'adadelta', 'adagrad']\n",
    "}\n",
    "\n",
    "# Define a custom scorer for GridSearchCV\n",
    "def mean_cosine_similarity(y_true, y_pred):\n",
    "    cosine_similarities = cosine_similarity(y_true, y_pred)\n",
    "    return np.mean(cosine_similarities)\n",
    "\n",
    "cosine_similarity_scorer = make_scorer(mean_cosine_similarity, greater_is_better=True)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=autoencoder, param_grid=param_grid, scoring=cosine_similarity_scorer, cv=3)\n",
    "grid_result = grid_search.fit(X_train, X_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters: \", grid_result.best_params_)\n",
    "print(\"Best Score: \", grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "69/69 [==============================] - 7s 64ms/step - loss: 0.5072 - val_loss: 0.2547\n",
      "Epoch 2/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.5030 - val_loss: 0.2524\n",
      "Epoch 3/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.4980 - val_loss: 0.2490\n",
      "Epoch 4/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.4900 - val_loss: 0.2433\n",
      "Epoch 5/50\n",
      "69/69 [==============================] - 3s 51ms/step - loss: 0.4752 - val_loss: 0.2327\n",
      "Epoch 6/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.4487 - val_loss: 0.2160\n",
      "Epoch 7/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.4072 - val_loss: 0.1911\n",
      "Epoch 8/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.3510 - val_loss: 0.1598\n",
      "Epoch 9/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.2854 - val_loss: 0.1257\n",
      "Epoch 10/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.2194 - val_loss: 0.0942\n",
      "Epoch 11/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.1611 - val_loss: 0.0678\n",
      "Epoch 12/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.1152 - val_loss: 0.0493\n",
      "Epoch 13/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.0818 - val_loss: 0.0361\n",
      "Epoch 14/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0586 - val_loss: 0.0272\n",
      "Epoch 15/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.0430 - val_loss: 0.0212\n",
      "Epoch 16/50\n",
      "69/69 [==============================] - 4s 57ms/step - loss: 0.0326 - val_loss: 0.0173\n",
      "Epoch 17/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.0258 - val_loss: 0.0147\n",
      "Epoch 18/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0212 - val_loss: 0.0130\n",
      "Epoch 19/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0182 - val_loss: 0.0119\n",
      "Epoch 20/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0161 - val_loss: 0.0111\n",
      "Epoch 21/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.0147 - val_loss: 0.0106\n",
      "Epoch 22/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0137 - val_loss: 0.0102\n",
      "Epoch 23/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.0130 - val_loss: 0.0099\n",
      "Epoch 24/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0125 - val_loss: 0.0097\n",
      "Epoch 25/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.0121 - val_loss: 0.0095\n",
      "Epoch 26/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0118 - val_loss: 0.0094\n",
      "Epoch 27/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.0115 - val_loss: 0.0093\n",
      "Epoch 28/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0113 - val_loss: 0.0092\n",
      "Epoch 29/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0111 - val_loss: 0.0091\n",
      "Epoch 30/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.0110 - val_loss: 0.0090\n",
      "Epoch 31/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0109 - val_loss: 0.0090\n",
      "Epoch 32/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0107 - val_loss: 0.0089\n",
      "Epoch 33/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.0107 - val_loss: 0.0089\n",
      "Epoch 34/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0106 - val_loss: 0.0088\n",
      "Epoch 35/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0105 - val_loss: 0.0088\n",
      "Epoch 36/50\n",
      "69/69 [==============================] - 4s 54ms/step - loss: 0.0104 - val_loss: 0.0088\n",
      "Epoch 37/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0104 - val_loss: 0.0087\n",
      "Epoch 38/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.0103 - val_loss: 0.0087\n",
      "Epoch 39/50\n",
      "69/69 [==============================] - 4s 53ms/step - loss: 0.0103 - val_loss: 0.0087\n",
      "Epoch 40/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0102 - val_loss: 0.0087\n",
      "Epoch 41/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0102 - val_loss: 0.0086\n",
      "Epoch 42/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0101 - val_loss: 0.0086\n",
      "Epoch 43/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0101 - val_loss: 0.0086\n",
      "Epoch 44/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0101 - val_loss: 0.0086\n",
      "Epoch 45/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.0101 - val_loss: 0.0086\n",
      "Epoch 46/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0100 - val_loss: 0.0086\n",
      "Epoch 47/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0100 - val_loss: 0.0085\n",
      "Epoch 48/50\n",
      "69/69 [==============================] - 3s 51ms/step - loss: 0.0100 - val_loss: 0.0085\n",
      "Epoch 49/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0100 - val_loss: 0.0085\n",
      "Epoch 50/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0099 - val_loss: 0.0085\n",
      "69/69 [==============================] - 0s 5ms/step\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "18/18 [==============================] - 0s 9ms/step\n",
      "Mean Cosine Similarity: 0.3918759372627433\n"
     ]
    }
   ],
   "source": [
    "# Define the autoencoder architecture with increased complexity, regularization, and batch normalization\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 64  # Adjust as needed\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "# Add a dense layer with ReLU activation and batch normalization\n",
    "encoder_layer1 = Dense(128, activation='relu')(input_layer)\n",
    "encoder_layer1 = BatchNormalization()(encoder_layer1)\n",
    "# Add a dropout layer for regularization\n",
    "encoder_layer1 = Dropout(0.5)(encoder_layer1)\n",
    "\n",
    "# Add another dense layer with ReLU activation and batch normalization\n",
    "encoder_layer2 = Dense(encoding_dim, activation='relu')(encoder_layer1)\n",
    "encoder_layer2 = BatchNormalization()(encoder_layer2)\n",
    "# Add a dropout layer for regularization\n",
    "encoder_layer2 = Dropout(0.5)(encoder_layer2)\n",
    "\n",
    "decoder_layer1 = Dense(128, activation='relu')(encoder_layer2)\n",
    "decoder_layer1 = BatchNormalization()(decoder_layer1)\n",
    "\n",
    "decoder_layer2 = Dense(input_dim, activation='sigmoid')(decoder_layer1)\n",
    "# Add a dropout layer for regularization\n",
    "decoder_layer2 = Dropout(0.5)(decoder_layer2)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(input_layer, decoder_layer2)\n",
    "\n",
    "# Compile the autoencoder model with RMSprop optimizer\n",
    "autoencoder.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, validation_data=(X_test, X_test))\n",
    "\n",
    "# Extract features using the encoder part of the autoencoder\n",
    "encoder = Model(input_layer, encoder_layer2)\n",
    "X_encoded_train = encoder.predict(X_train)\n",
    "X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "# Reconstruct data using the trained autoencoder\n",
    "reconstructed_data = autoencoder.predict(X_test)\n",
    "\n",
    "# Combine original test data with reconstructed data\n",
    "X_test_combined = np.concatenate((X_test, reconstructed_data), axis=1)\n",
    "\n",
    "# Compute cosine similarity between original and reconstructed data samples\n",
    "cosine_similarities = cosine_similarity(X_test_combined)\n",
    "\n",
    "# Calculate the mean cosine similarity across all samples\n",
    "mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "print(\"Mean Cosine Similarity:\", mean_cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate schedule: constant\n",
      "Epoch 1/50\n",
      "69/69 [==============================] - 7s 62ms/step - loss: 0.5072 - val_loss: 0.2547 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "69/69 [==============================] - 3s 44ms/step - loss: 0.5030 - val_loss: 0.2525 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "69/69 [==============================] - 4s 57ms/step - loss: 0.4980 - val_loss: 0.2493 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "69/69 [==============================] - 3s 44ms/step - loss: 0.4900 - val_loss: 0.2436 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.4750 - val_loss: 0.2326 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "69/69 [==============================] - 4s 57ms/step - loss: 0.4482 - val_loss: 0.2149 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.4065 - val_loss: 0.1898 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "69/69 [==============================] - 3s 42ms/step - loss: 0.3502 - val_loss: 0.1586 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "69/69 [==============================] - 3s 50ms/step - loss: 0.2845 - val_loss: 0.1253 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.2184 - val_loss: 0.0940 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "69/69 [==============================] - 5s 73ms/step - loss: 0.1602 - val_loss: 0.0686 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "69/69 [==============================] - 4s 58ms/step - loss: 0.1143 - val_loss: 0.0493 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.0811 - val_loss: 0.0358 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "69/69 [==============================] - 3s 45ms/step - loss: 0.0581 - val_loss: 0.0270 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "69/69 [==============================] - 3s 51ms/step - loss: 0.0426 - val_loss: 0.0210 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0323 - val_loss: 0.0172 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.0256 - val_loss: 0.0147 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.0210 - val_loss: 0.0130 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "69/69 [==============================] - 3s 45ms/step - loss: 0.0180 - val_loss: 0.0119 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "69/69 [==============================] - 4s 61ms/step - loss: 0.0160 - val_loss: 0.0111 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "69/69 [==============================] - 3s 45ms/step - loss: 0.0147 - val_loss: 0.0106 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "69/69 [==============================] - 3s 44ms/step - loss: 0.0137 - val_loss: 0.0102 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0130 - val_loss: 0.0099 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.0125 - val_loss: 0.0097 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "69/69 [==============================] - 3s 48ms/step - loss: 0.0121 - val_loss: 0.0095 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "69/69 [==============================] - 4s 54ms/step - loss: 0.0117 - val_loss: 0.0094 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "69/69 [==============================] - 4s 61ms/step - loss: 0.0115 - val_loss: 0.0093 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "69/69 [==============================] - 4s 62ms/step - loss: 0.0113 - val_loss: 0.0092 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "69/69 [==============================] - 5s 68ms/step - loss: 0.0111 - val_loss: 0.0091 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "69/69 [==============================] - 4s 59ms/step - loss: 0.0110 - val_loss: 0.0090 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "69/69 [==============================] - 4s 62ms/step - loss: 0.0108 - val_loss: 0.0090 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "69/69 [==============================] - 6s 89ms/step - loss: 0.0107 - val_loss: 0.0089 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "69/69 [==============================] - 5s 73ms/step - loss: 0.0106 - val_loss: 0.0089 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "69/69 [==============================] - 3s 51ms/step - loss: 0.0106 - val_loss: 0.0088 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "69/69 [==============================] - 5s 65ms/step - loss: 0.0105 - val_loss: 0.0088 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "69/69 [==============================] - 3s 41ms/step - loss: 0.0104 - val_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "69/69 [==============================] - 3s 43ms/step - loss: 0.0104 - val_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0103 - val_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "69/69 [==============================] - 4s 51ms/step - loss: 0.0103 - val_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "69/69 [==============================] - 4s 55ms/step - loss: 0.0102 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "69/69 [==============================] - 4s 52ms/step - loss: 0.0102 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "69/69 [==============================] - 5s 67ms/step - loss: 0.0101 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0101 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "69/69 [==============================] - 3s 49ms/step - loss: 0.0101 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "69/69 [==============================] - 4s 58ms/step - loss: 0.0100 - val_loss: 0.0085 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "69/69 [==============================] - 4s 55ms/step - loss: 0.0100 - val_loss: 0.0085 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "69/69 [==============================] - 4s 61ms/step - loss: 0.0100 - val_loss: 0.0085 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.0100 - val_loss: 0.0085 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "69/69 [==============================] - 4s 60ms/step - loss: 0.0099 - val_loss: 0.0085 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "69/69 [==============================] - 4s 55ms/step - loss: 0.0099 - val_loss: 0.0085 - lr: 0.0010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m schedule_name, schedule_func \u001b[38;5;129;01min\u001b[39;00m schedules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with learning rate schedule:\u001b[39m\u001b[38;5;124m\"\u001b[39m, schedule_name)\n\u001b[0;32m---> 80\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschedule_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Cosine Similarity:\u001b[39m\u001b[38;5;124m\"\u001b[39m, similarity)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m best_similarity:\n",
      "Cell \u001b[0;32mIn[18], line 50\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(X_train, X_test, schedule)\u001b[0m\n\u001b[1;32m     47\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m LearningRateScheduler(schedule)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Train the autoencoder with learning rate scheduler\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Extract features using the encoder part of the autoencoder\u001b[39;00m\n\u001b[1;32m     54\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Model(input_layer, encoder_layer2)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "\n",
    "# Define the learning rate schedules\n",
    "schedules = {\n",
    "    'constant': lambda epoch: 0.001,\n",
    "    'decreasing': lambda epoch: 0.001 * np.exp(-0.1 * epoch),\n",
    "    'adaptive': lambda epoch: 0.001 if epoch < 10 else 0.0005\n",
    "}\n",
    "\n",
    "# Define a function to train the autoencoder with a given learning rate schedule\n",
    "def train_autoencoder(X_train, X_test, schedule):\n",
    "    # Define the autoencoder architecture\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = 64  # Adjust as needed\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder_layer1 = Dense(128, activation='relu')(input_layer)\n",
    "    encoder_layer1 = BatchNormalization()(encoder_layer1)\n",
    "    encoder_layer1 = Dropout(0.5)(encoder_layer1)\n",
    "\n",
    "    encoder_layer2 = Dense(encoding_dim, activation='relu')(encoder_layer1)\n",
    "    encoder_layer2 = BatchNormalization()(encoder_layer2)\n",
    "    encoder_layer2 = Dropout(0.5)(encoder_layer2)\n",
    "\n",
    "    decoder_layer1 = Dense(128, activation='relu')(encoder_layer2)\n",
    "    decoder_layer1 = BatchNormalization()(decoder_layer1)\n",
    "\n",
    "    decoder_layer2 = Dense(input_dim, activation='sigmoid')(decoder_layer1)\n",
    "    decoder_layer2 = Dropout(0.5)(decoder_layer2)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoder_layer2)\n",
    "\n",
    "    # Define the optimizer with RMSprop\n",
    "    optimizer = RMSprop(learning_rate=0.001) \n",
    "\n",
    "    # Compile the autoencoder model with RMSprop optimizer\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Define the learning rate scheduler callback\n",
    "    lr_scheduler = LearningRateScheduler(schedule)\n",
    "\n",
    "    # Train the autoencoder with learning rate scheduler\n",
    "    autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, \n",
    "                    validation_data=(X_test, X_test), callbacks=[lr_scheduler])\n",
    "\n",
    "    # Extract features using the encoder part of the autoencoder\n",
    "    encoder = Model(input_layer, encoder_layer2)\n",
    "    X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "    # Reconstruct data using the trained autoencoder\n",
    "    reconstructed_data = autoencoder.predict(X_test)\n",
    "\n",
    "    # Combine original test data with reconstructed data\n",
    "    X_test_combined = np.concatenate((X_test, reconstructed_data), axis=1)\n",
    "\n",
    "    # Compute cosine similarity between original and reconstructed data samples\n",
    "    cosine_similarities = cosine_similarity(X_test_combined)\n",
    "\n",
    "    # Calculate the mean cosine similarity across all samples\n",
    "    mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "    \n",
    "    return mean_cosine_similarity\n",
    "\n",
    "# Assuming you have your data stored in X_processed and y_encoded\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the autoencoder with different learning rate schedules and keep track of the best one\n",
    "best_schedule = None\n",
    "best_similarity = -1\n",
    "for schedule_name, schedule_func in schedules.items():\n",
    "    print(\"Training with learning rate schedule:\", schedule_name)\n",
    "    similarity = train_autoencoder(X_train, X_test, schedule_func)\n",
    "    print(\"Mean Cosine Similarity:\", similarity)\n",
    "    if similarity > best_similarity:\n",
    "        best_similarity = similarity\n",
    "        best_schedule = schedule_name\n",
    "\n",
    "print(\"Best learning rate schedule:\", best_schedule)\n",
    "print(\"Best Mean Cosine Similarity:\", best_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decreasing_schedule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_processed, y_encoded, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Train the autoencoder with the decreasing learning rate schedule and RMSprop optimizer\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Cosine Similarity:\u001b[39m\u001b[38;5;124m\"\u001b[39m, similarity)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Extract features using the encoder part of the autoencoder\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 39\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(X_train, X_test)\u001b[0m\n\u001b[1;32m     36\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Define the learning rate scheduler callback\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m LearningRateScheduler(\u001b[43mdecreasing_schedule\u001b[49m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Train the autoencoder with learning rate scheduler\u001b[39;00m\n\u001b[1;32m     42\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mfit(X_train, X_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     43\u001b[0m                 validation_data\u001b[38;5;241m=\u001b[39m(X_test, X_test), callbacks\u001b[38;5;241m=\u001b[39m[lr_scheduler])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decreasing_schedule' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def train_autoencoder(X_train, X_test):\n",
    "    # Define the autoencoder architecture\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = 64  # Adjust as needed\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder_layer1 = Dense(128, activation='relu')(input_layer)\n",
    "    encoder_layer1 = BatchNormalization()(encoder_layer1)\n",
    "    encoder_layer1 = Dropout(0.5)(encoder_layer1)\n",
    "\n",
    "    encoder_layer2 = Dense(encoding_dim, activation='relu')(encoder_layer1)\n",
    "    encoder_layer2 = BatchNormalization()(encoder_layer2)\n",
    "    encoder_layer2 = Dropout(0.5)(encoder_layer2)\n",
    "\n",
    "    decoder_layer1 = Dense(128, activation='relu')(encoder_layer2)\n",
    "    decoder_layer1 = BatchNormalization()(decoder_layer1)\n",
    "\n",
    "    decoder_layer2 = Dense(input_dim, activation='sigmoid')(decoder_layer1)  # Adjusted output dimensionality\n",
    "    decoder_layer2 = Dropout(0.5)(decoder_layer2)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoder_layer2)\n",
    "\n",
    "    # Define the optimizer with RMSprop\n",
    "    optimizer = RMSprop(learning_rate=0.001) \n",
    "\n",
    "    # Compile the autoencoder model with RMSprop optimizer\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Define the learning rate scheduler callback\n",
    "    lr_scheduler = LearningRateScheduler(decreasing_schedule)\n",
    "\n",
    "    # Train the autoencoder with learning rate scheduler\n",
    "    autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, \n",
    "                    validation_data=(X_test, X_test), callbacks=[lr_scheduler])\n",
    "\n",
    "    # Extract features using the encoder part of the autoencoder\n",
    "    encoder = Model(input_layer, encoder_layer2)\n",
    "    X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "    # Reconstruct data using the trained autoencoder\n",
    "    reconstructed_data = autoencoder.predict(X_test)\n",
    "\n",
    "    # Combine original test data with reconstructed data\n",
    "    X_test_combined = np.concatenate((X_test, reconstructed_data), axis=1)\n",
    "\n",
    "    # Compute cosine similarity between original and reconstructed data samples\n",
    "    cosine_similarities = cosine_similarity(X_test_combined)\n",
    "\n",
    "    # Calculate the mean cosine similarity across all samples\n",
    "    mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "    \n",
    "    return mean_cosine_similarity\n",
    "\n",
    "# Assuming you have your data stored in X_processed and y_encoded\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the autoencoder with the decreasing learning rate schedule and RMSprop optimizer\n",
    "similarity = train_autoencoder(X_train, X_test)\n",
    "print(\"Mean Cosine Similarity:\", similarity)\n",
    "\n",
    "# Extract features using the encoder part of the autoencoder\n",
    "encoder = Model(input_layer, encoder_layer2)\n",
    "X_encoded_train = encoder.predict(X_train)\n",
    "X_encoded_test = encoder.predict(X_test)\n",
    "\n",
    "# Define the neural network model\n",
    "input_shape_nn = X_encoded_train.shape[1]\n",
    "model_nn = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(input_shape_nn,)),  \n",
    "    Dense(64, activation='relu'),  \n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the neural network model\n",
    "model_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the neural network model using the encoded features\n",
    "model_nn.fit(X_encoded_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "# Evaluate the neural network model\n",
    "loss, accuracy = model_nn.evaluate(X_encoded_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
